{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Xvector embeddings generation\n",
    "output-file: xvector_embeddings.html\n",
    "description: Loads xvector embeddings from audio streams\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp xvector_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class AnnoyHandler(object):\n",
    "    \n",
    "    def __init__(self, dimentions: int, index_path: str = None):\n",
    "        if not dimentions:\n",
    "            raise ValueError(f\"Need dimentions, got '{dimentions}'\")\n",
    "        self.dimentions = dimentions\n",
    "        self.index = AnnoyIndex(self.dimentions, 'angular')\n",
    "        self.id2label = {}\n",
    "        if index_path:\n",
    "            self.index.load(index_path)\n",
    "            dir = Path(index_path).parent\n",
    "            name = Path(index_path).stem + '.json'\n",
    "            with open(os.path.join(dir, name), 'r') as f:\n",
    "                self.id2label = json.load(f)\n",
    "            self.items = len(self.id2label)\n",
    "        self.index.set_seed(1991)\n",
    "    \n",
    "    def add_item(self, vector, label) -> None:\n",
    "        i = len(self.id2label)\n",
    "        self.id2label[i] = label if label else None\n",
    "        self.index.add_item(i, vector)\n",
    "\n",
    "    def build(self, trees: int = 10):\n",
    "        return self.index.build(trees, n_jobs=4)\n",
    "    \n",
    "    def unbuild(self):\n",
    "        self.index.unbuild()\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        self.index.save(path)\n",
    "        dir = Path(path).parent\n",
    "        name = Path(path).stem + '.json'\n",
    "        with open(os.path.join(dir, name), 'w') as f:\n",
    "            json.dump(self.id2label, f)\n",
    "    \n",
    "    def get_nns(self, vector, n: int):\n",
    "        indexes, similarities = self.index.get_nns_by_vector(vector, n, include_distances=True)\n",
    "        labels = [self.id2label[str(i)] for i in indexes]\n",
    "        return indexes, similarities, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from transformers import Wav2Vec2ForXVector\n",
    "from wav2keyword.audio_processor import AudioProcessor\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from random import choices\n",
    "import torch\n",
    "\n",
    "class AudioArray(BaseModel):\n",
    "    array: np.ndarray\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class XvectorModel(object):\n",
    "\n",
    "    def __init__(self, model_checkpoint: str, annoy_index_path: str = None) -> None:\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.model = Wav2Vec2ForXVector.from_pretrained(self.model_checkpoint)\n",
    "        self.audio_processor = AudioProcessor(self.model_checkpoint)\n",
    "        self.embeddings_dimention = 512\n",
    "        self.annoy_handler = AnnoyHandler(self.embeddings_dimention, annoy_index_path)\n",
    "    \n",
    "    def prepare_raw_audio(self, raw_data: bytes, sample_width: int, channels: int, frame_rate: int):\n",
    "        return self.audio_processor.encode_raw_audio(raw_data, sample_width, channels, frame_rate)\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            result = self.model(**inputs).embeddings\n",
    "        return result\n",
    "\n",
    "    def get_predicted_labels(self, logits):\n",
    "        proj = self.model.objective._parameters['weight'].cpu().detach().numpy()\n",
    "        return np.argmax(np.dot(logits, proj), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5255e-04, -3.4512e-03, -3.3339e-04,  ...,  9.5668e-04,\n",
      "          2.7784e-03, -9.6302e-04],\n",
      "        [ 8.0288e-04, -1.0105e-03, -2.0560e-04,  ..., -4.4576e-04,\n",
      "         -4.0969e-04,  2.0239e-03],\n",
      "        [ 1.3787e-04, -4.7847e-04,  6.4930e-04,  ...,  9.0482e-05,\n",
      "          2.8542e-04,  9.5453e-04],\n",
      "        [ 8.3139e-04, -1.1354e-03,  1.1984e-03,  ..., -9.5183e-05,\n",
      "          3.6603e-04,  8.8071e-05]])\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "model_checkpoint = 'data/panda/wav2vec2-base-finetuned-xvector/best_checkpoint/'\n",
    "xvector_model = XvectorModel(model_checkpoint)\n",
    "\n",
    "files = glob(f'{Path.home()}/.cache/panda/audios/*.wav')\n",
    "file = choices(files, k=1)[0]\n",
    "with open(file, \"rb\") as f:\n",
    "    audio_bytes = bytearray()\n",
    "    while (byte := f.read(1)):\n",
    "        audio_bytes.extend(byte)\n",
    "encoded_data = xvector_model.prepare_raw_audio(audio_bytes, 2, 2, 16000)\n",
    "embeddings = xvector_model.get_embeddings(encoded_data)\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
