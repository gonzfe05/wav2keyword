[
  {
    "objectID": "datasets_tools.html",
    "href": "datasets_tools.html",
    "title": "Datasets pipeline",
    "section": "",
    "text": "Code\n\nsource\n\ndataloader_pipeline\n\n dataloader_pipeline (load_dataset_kwargs:dict)\n\nLoads dataset and builds maps from label to id and id to label.\nArgs: load_dataset_kwargs (dict): Parameters for huggingfaceâ€™s load_dataset\nReturns: Dict: Keys are â€˜datasetâ€™, â€˜id2labelâ€™ and â€˜label2idâ€™\n\nassert _get_labels_dict(['a', 'b', 'c'])[0] == {'0': 'a', '1': 'b', '2': 'c'}\nassert _get_labels_dict(['a', 'b', 'c'])[1] == {'a': '0', 'b': '1', 'c': '2'}\ndata = dataloader_pipeline({'path': \"superb\", 'name': \"ks\"})\nassert isinstance(data['dataset'], DatasetDict)\n\nReusing dataset superb (/home/jovyan/.cache/huggingface/datasets/superb/ks/1.9.0/ce836692657f82230c16b3bbcb93eaacdbfd7de4def3be90016f112d68683481)\n\n\n\n\n\n\n\n\nExample\nIn this example, we load data for a Keyword Spotting task of the SUPERB Benchmark\nKeyword Spotting (KS) detects preregistered keywords by classifying utterances into a predefined set of words. SUPERB uses the widely used Speech Commands dataset v1.0 for the task.\nThe dataset consists of ten classes of keywords, a class for silence, and an unknown class to include the false positive.\nWe will wrap the ðŸ¤— Datasets library to download the data\n\ndata = dataloader_pipeline({'path': \"superb\", 'name': \"ks\"})\ndataset = data['dataset'] #type: DatasetDict\nid2label = data['id2label'] #type: Dict\nlabel2id = data['label2id'] #type: Dict\n\n\n\n\n\n\n\nThe dataset object itself is a DatasetDict, which contains one key for the training, validation and test set.\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 51094\n    })\n    validation: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 6798\n    })\n    test: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 3081\n    })\n})\n\n\nTo access an actual element, you need to select a split first, then give an index:\n\ndataset[\"test\"][1000]\n\n{'file': '/home/jovyan/.cache/huggingface/datasets/downloads/extracted/45bfc1522471574e4f4c96727b6574012e571193a6708acd3bb51160d97475fd/go/e41a903b_nohash_4.wav',\n 'audio': {'path': '/home/jovyan/.cache/huggingface/datasets/downloads/extracted/45bfc1522471574e4f4c96727b6574012e571193a6708acd3bb51160d97475fd/go/e41a903b_nohash_4.wav',\n  'array': array([-1.2207031e-04,  3.0517578e-05,  1.8310547e-04, ...,\n         -4.8828125e-04, -5.4931641e-04, -3.3569336e-04], dtype=float32),\n  'sampling_rate': 16000},\n 'label': 9}\n\n\nAs you can see, the label field is not an actual string label. By default the ClassLabel fields are encoded into integers for convenience:\n\ndataset[\"train\"].features[\"label\"]\n\nClassLabel(num_classes=12, names=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', '_silence_', '_unknown_'], names_file=None, id=None)\n\n\nWe can see that the audio file has been loaded and resampled on-the-fly upon calling.\nThe sampling rate is set to 16kHz which is what Wav2Vec2 expects as an input.\nTo get a sense of what the commands sound like, the following snippet will render some audio examples picked randomly from the dataset.\nNote: You can run the following cell a couple of times to listen to different audio samples.\n\nfor _ in range(5):\n    rand_idx = random.randint(0, len(dataset[\"train\"])-1)\n    example = dataset[\"train\"][rand_idx]\n    audio = example[\"audio\"]\n    label = str(example[\"label\"])\n    print(f'Label: {id2label[label]}')\n    print(f'Shape: {audio[\"array\"].shape}, sampling rate: {audio[\"sampling_rate\"]}')\n    display(Audio(audio[\"array\"], rate=audio[\"sampling_rate\"]))\n    print()\n\nLabel: left\nShape: (16000,), sampling rate: 16000\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nLabel: no\nShape: (16000,), sampling rate: 16000\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nLabel: right\nShape: (16000,), sampling rate: 16000\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nLabel: _unknown_\nShape: (16000,), sampling rate: 16000\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nLabel: down\nShape: (16000,), sampling rate: 16000\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "06_embeddings_distance.html",
    "href": "06_embeddings_distance.html",
    "title": "Wav2vec2 embeddings distance",
    "section": "",
    "text": "from IPython.display import display, Audio\nfrom glob import glob\nimport json\nfrom datasets import load_dataset, Features, Value, Audio, ClassLabel\nimport random\nWe already trained and run a model. The results over the test set are in the emb_sim folder.\nWe precomputed the wav2vec2 embeddings for all the audios. The siamese network was fed pairs of embeddings and predicted if the audios had the same work spoken or not.\nIn the emb_sim folder we saved the results for the testing dataset. For each audio pair in the test dataset, if the model judges the pair had the same word spoken, we saved both audios under a subfolder.\nThe subfolder is the transcripted word for the first audio in the pair. It is important to note that in this test, we compared pairs of audio the model didnt train on. We expect better results if we compare against trained audios."
  },
  {
    "objectID": "06_embeddings_distance.html#sample-of-correct-examples-found",
    "href": "06_embeddings_distance.html#sample-of-correct-examples-found",
    "title": "Wav2vec2 embeddings distance",
    "section": "Sample of correct examples found",
    "text": "Sample of correct examples found\n\nSame pairs\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(correct_test_files_same)-1)\n    example = correct_test_files_same[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nDifferent pairs\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(correct_test_files_dif)-1)\n    example = correct_test_files_dif[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "06_embeddings_distance.html#sample-of-incorrect-examples-found",
    "href": "06_embeddings_distance.html#sample-of-incorrect-examples-found",
    "title": "Wav2vec2 embeddings distance",
    "section": "Sample of incorrect examples found",
    "text": "Sample of incorrect examples found\n\nSame pairs\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(incorrect_test_files_same)-1)\n    example = incorrect_test_files_same[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nDifferent pairs\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(incorrect_test_files_dif)-1)\n    example = incorrect_test_files_dif[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wav2keyword",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "wav2keyword",
    "section": "Install",
    "text": "Install\npip install wav2keyword"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "wav2keyword",
    "section": "How to use",
    "text": "How to use\nFoobar examples:\n\nfoo('a', 'b')\n\n'foobar'"
  },
  {
    "objectID": "data/panda/chow_mein_ks.html",
    "href": "data/panda/chow_mein_ks.html",
    "title": "wav2keyword",
    "section": "",
    "text": "from datasets import load_dataset, Features, Value, Audio, ClassLabel\nimport json\n\nfeats = Features({\"path\": Value(\"string\"),\n                  \"audio\": Audio(sampling_rate=16_000),\n                  \"label\": ClassLabel(names=[\"not found\",\"found\"])}\n                  )\ndef _generate_examples(example, tag):\n        example['label'] = 1 if example['label'] in tag else 0\n        example['audio'] = example['path']\n        return example\n\nwith open('tags_data.json', 'r') as f:\n    data = json.load(f)\n\ndata_files = {'train': 'dataset/slices_train.csv', 'test': 'dataset/slices_test.csv', 'val': 'dataset/slices_val.csv'}\ndataset = load_dataset(\"csv\", data_files=data_files)\ndataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\ntags_pool = [k for k, v in data.items() if 'chow mein' in v['tags']]\ndataset = dataset.map(_generate_examples, fn_kwargs={'tag': tags_pool}, features=feats)\ndataset = dataset.rename_column('path', 'file')\nid2label = {0: 'not found', 1: 'found'}\nlabel2id = {v: k for k, v in id2label.items()}\n\nUsing custom data configuration default-c58ed15a5d5a3dac\nReusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndataset['train'][0]\n\n{'file': '/home/jovyan/.cache/panda/audio_slices/water_0_1655689126-SIP-A90CCE12F2CF-000041b2-chunk3.wav',\n 'audio': {'path': '/home/jovyan/.cache/panda/audio_slices/water_0_1655689126-SIP-A90CCE12F2CF-000041b2-chunk3.wav',\n  'array': array([ 6.1035156e-05,  3.3569336e-04, -4.2724609e-04, ...,\n         -4.8522949e-03,  1.3031006e-02,  2.9037476e-02], dtype=float32),\n  'sampling_rate': 16000},\n 'label': 0}\n\n\n\ndataset['train'].to_pandas().label.value_counts()\n\n0    35787\n1     1206\nName: label, dtype: int64\n\n\n\ndef _filter_by_duration(example, duration):\n    return len(example['audio']['array']) < duration * example['audio']['sampling_rate']\n\ndataset = dataset.filter(_filter_by_duration, fn_kwargs={'duration': 1})\n\n\n\n\n\n\n\n\n\n\n\nclass_counts = dataset['train'].to_pandas().label.value_counts()\nweight_positive_class = class_counts.iloc[0]/class_counts.iloc[1]\nprint(weight_positive_class)\n\n30.22027972027972\n\n\n\nfrom transformers import AutoFeatureExtractor\nfrom transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\n\nmodel_checkpoint = \"facebook/wav2vec2-base\"\nbatch_size = 32\nmax_duration = 1\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, \n        sampling_rate=feature_extractor.sampling_rate, \n        max_length=int(feature_extractor.sampling_rate * max_duration), \n        truncation=True, \n    )\n    return inputs\n\nencoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)\n\nnum_labels = len(id2label)\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    model_checkpoint, \n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label,\n)\n\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-ks\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=15,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n)\n\nloading feature extractor configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/preprocessor_config.json from cache at /home/jovyan/.cache/huggingface/transformers/d4583dd9e59eb6295f8fe8b18833ae54d963a122d69aa1df7ecce6caafe18c8f.bc3155ca0bae3a39fc37fc6d64829c6a765f46480894658bb21c08db6155358d\nloading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\nModel config Wav2Vec2Config {\n  \"_name_or_path\": \"facebook/wav2vec2-base\",\n  \"activation_dropout\": 0.0,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForPreTraining\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": false,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"sum\",\n  \"ctc_zero_infinity\": false,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": false,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_norm\": \"group\",\n  \"feat_proj_dropout\": 0.1,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.0,\n  \"freeze_feat_extract_train\": true,\n  \"gradient_checkpointing\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.0,\n  \"mask_channel_length\": 10,\n  \"mask_channel_min_space\": 1,\n  \"mask_channel_other\": 0.0,\n  \"mask_channel_prob\": 0.0,\n  \"mask_channel_selection\": \"static\",\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_min_space\": 1,\n  \"mask_time_other\": 0.0,\n  \"mask_time_prob\": 0.05,\n  \"mask_time_selection\": \"static\",\n  \"model_type\": \"wav2vec2\",\n  \"no_mask_channel_overlap\": false,\n  \"no_mask_time_overlap\": false,\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 12,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 12,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 768,\n  \"pad_token_id\": 0,\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.21.2\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 32,\n  \"xvector_output_dim\": 512\n}\n\nFeature extractor Wav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}\n\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n\n\n\n\n\n\n\n\nloading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\nModel config Wav2Vec2Config {\n  \"_name_or_path\": \"facebook/wav2vec2-base\",\n  \"activation_dropout\": 0.0,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForPreTraining\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": false,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"sum\",\n  \"ctc_zero_infinity\": false,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": false,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_norm\": \"group\",\n  \"feat_proj_dropout\": 0.1,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.0,\n  \"freeze_feat_extract_train\": true,\n  \"gradient_checkpointing\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"not found\",\n    \"1\": \"found\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"found\": 1,\n    \"not found\": 0\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.0,\n  \"mask_channel_length\": 10,\n  \"mask_channel_min_space\": 1,\n  \"mask_channel_other\": 0.0,\n  \"mask_channel_prob\": 0.0,\n  \"mask_channel_selection\": \"static\",\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_min_space\": 1,\n  \"mask_time_other\": 0.0,\n  \"mask_time_prob\": 0.05,\n  \"mask_time_selection\": \"static\",\n  \"model_type\": \"wav2vec2\",\n  \"no_mask_channel_overlap\": false,\n  \"no_mask_time_overlap\": false,\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 12,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 12,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 768,\n  \"pad_token_id\": 0,\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.21.2\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 32,\n  \"xvector_output_dim\": 512\n}\n\nloading weights file https://huggingface.co/facebook/wav2vec2-base/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a\nSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.weight_proj.weight']\n- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'projector.bias', 'classifier.weight', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n\n\n\nmetric = load_metric(\"accuracy\")\n\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (2 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.35]).cuda())\n        logits_view = logits.view(-1, self.model.config.num_labels)\n        loss = loss_fct(logits_view, labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = CustomTrainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"val\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics\n)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 35708\n  Num Epochs = 15\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 4\n  Total optimization steps = 4185\n\n\n\n\n    \n      \n      \n      [4185/4185 2:23:46, Epoch 15/15]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Accuracy\n    \n  \n  \n    \n      1\n      0.411200\n      0.306926\n      0.882207\n    \n    \n      2\n      0.283400\n      0.227008\n      0.904730\n    \n    \n      3\n      0.242800\n      0.193498\n      0.914414\n    \n    \n      4\n      0.221000\n      0.166870\n      0.923874\n    \n    \n      5\n      0.227800\n      0.159021\n      0.926802\n    \n    \n      6\n      0.197600\n      0.158407\n      0.931081\n    \n    \n      7\n      0.207700\n      0.141965\n      0.934685\n    \n    \n      8\n      0.181700\n      0.148205\n      0.929054\n    \n    \n      9\n      0.172800\n      0.138138\n      0.936036\n    \n    \n      10\n      0.186800\n      0.141322\n      0.933333\n    \n    \n      11\n      0.180400\n      0.133537\n      0.935811\n    \n    \n      12\n      0.170000\n      0.132791\n      0.934910\n    \n    \n      13\n      0.153300\n      0.133605\n      0.933784\n    \n    \n      14\n      0.126600\n      0.132347\n      0.935811\n    \n    \n      15\n      0.144800\n      0.129441\n      0.936486\n    \n  \n\n\n\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-279\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-279/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-279/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-279/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-558\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-558/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-558/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-558/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-837\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-837/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-837/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-837/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1116\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1116/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1116/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1116/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1395\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1395/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1395/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1395/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1674\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1674/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1674/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1674/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1953\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1953/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1953/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1953/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2232\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-2232/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-2232/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2232/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2511\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-2511/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-2511/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2511/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2790\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-2790/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-2790/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2790/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3069\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-3069/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-3069/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3069/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3348\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-3348/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-3348/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3348/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3627\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-3627/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-3627/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3627/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3906\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-3906/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-3906/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3906/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4440\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-4185\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-4185/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-4185/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-4185/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from wav2vec2-base-finetuned-ks/checkpoint-4185 (score: 0.9364864864864865).\n\n\nTrainOutput(global_step=4185, training_loss=0.21789302935594584, metrics={'train_runtime': 8628.3807, 'train_samples_per_second': 62.077, 'train_steps_per_second': 0.485, 'total_flos': 4.3021878417689375e+18, 'train_loss': 0.21789302935594584, 'epoch': 15.0})\n\n\n\ndataset.cleanup_cache_files()\n\n{'train': 5, 'test': 1, 'val': 0, 'validation': 0}\n\n\n\ntrainer.save_model(f\"wav2vec2-base-finetuned-ks/best_checkpoint\")\n\nSaving model checkpoint to wav2vec2-base-finetuned-ks/best_checkpoint\nConfiguration saved in wav2vec2-base-finetuned-ks/best_checkpoint/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/best_checkpoint/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/best_checkpoint/preprocessor_config.json\n\n\n\ninputs = encoded_dataset['test']\n\nwith torch.no_grad():\n        result = trainer.predict(test_dataset = inputs)\nresult\n\n***** Running Prediction *****\n  Num examples = 4479\n  Batch size = 32\n\n\n\n\n\nPredictionOutput(predictions=array([[ 1.2470611, -1.2712642],\n       [ 3.5061896, -3.6580167],\n       [ 3.2142575, -3.3506398],\n       ...,\n       [ 3.4247284, -3.5656137],\n       [ 3.067631 , -3.2232652],\n       [ 3.5443592, -3.709301 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.1558578908443451, 'test_accuracy': 0.9278856887698147, 'test_runtime': 28.6291, 'test_samples_per_second': 156.449, 'test_steps_per_second': 4.89})"
  },
  {
    "objectID": "data/panda/chow_mein_ks.html#chow-mein",
    "href": "data/panda/chow_mein_ks.html#chow-mein",
    "title": "wav2keyword",
    "section": "Chow mein",
    "text": "Chow mein\n\npredicted_labels = np.argmax(result.predictions, axis=-1)\npositive_hits = [p==t for p, t in zip(predicted_labels, inputs['label']) if t == 1]\ncorrect_positive = sum(positive_hits)\ncorrect_negative = sum([p==t for p, t in zip(predicted_labels, inputs['label']) if t == 0])\nn_positive = sum(inputs['label'])\nn_negative = len(inputs['label']) - n_positive\n\nprint(f\"Overall accuracy: {(correct_positive + correct_negative) / (n_positive + n_negative)}\")\nprint(f\"Positive accutacy: {correct_positive / n_positive}\")\nprint(f\"Negative accuracy: {correct_negative / n_negative}\")\n\nOverall accuracy: 0.9752176825184193\nPositive accutacy: 0.7633587786259542\nNegative accuracy: 0.9816007359705612\n\n\n\nimport shutil\nfrom pathlib import Path\n\npositive_hits_files = [f for p, t, f in zip(predicted_labels, dataset['test']['label'], dataset['test']['file']) if t == 1 and p==t]\nfor f in positive_hits_files:\n    shutil.copy(f, f\"found/{Path(f).name}\")\n\n\nimport shutil\nfrom pathlib import Path\n\npositive_miss_files = [f for p, t, f in zip(predicted_labels, dataset['test']['label'], dataset['test']['file']) if t == 1 and p!=t]\nfor f in positive_miss_files:\n    shutil.copy(f, f\"not found/{Path(f).name}\")"
  },
  {
    "objectID": "data/panda/Force alignment.html",
    "href": "data/panda/Force alignment.html",
    "title": "wav2keyword",
    "section": "",
    "text": "from praatio import textgrid\n\n# Textgrids take no arguments--it gets all of its necessary attributes from the tiers that it contains.\ntg = textgrid.Textgrid()\n\n# IntervalTiers and PointTiers take four arguments: the tier name, a list of intervals or points,\n# a starting time, and an ending time.\nwordTier = textgrid.IntervalTier('words', [(0,1,'a')], 0, 1.0)\n\n\nimport os\nfrom os.path import join\nfrom pathlib import Path\nimport re\n\nfrom praatio import textgrid\nfrom praatio import audio\n\nfrom tqdm import tqdm\n\n\ninputPath = {'audios': f'{Path.home()}/.cache/panda/audios/', 'transcripts': f'{Path.home()}/.cache/panda/transcripts/'}\noutputPath = f'{Path.home()}/.cache/panda/textgrids/'\n\nif not os.path.exists(outputPath):\n    os.mkdir(outputPath)\n\nv_ix = 0\nfor fn in tqdm(os.listdir(inputPath['audios'])):\n    v_ix += 1\n    name, ext = os.path.splitext(fn)\n    if ext != \".wav\":\n        continue\n    duration = audio.getDuration(join(inputPath['audios'], fn))\n    with open(join(inputPath['transcripts'], f\"{name}.txt\"), 'r') as f:\n        text = f.read()\n        utterances = text.split('\\n')\n        try:\n            voices = [re.search('\\[(.+?)\\]', u.replace('[unsure:]', '---')).group(1).replace(':', '') for u in utterances if u]\n            voices = [f\"{v}_{v_ix}\" for v in voices]\n        except:\n            print(utterances)\n            for u in utterances:\n                print(u)\n                print(re.search('\\[(.+?)\\]', u.replace('[unsure:]', '---')))\n            break\n        utterances = [re.sub('\\[.+?\\]|-|\\.', '', u).strip() for u in utterances]\n        ix = 0\n        tiers = {}\n        for k, utt in zip(voices, utterances):\n            end = ix+0.3\n            tiers[k] = tiers.get(k, []) + [(ix, end, utt)]\n            ix = end + 0.1\n    tg = textgrid.Textgrid()\n    for k, utt in tiers.items():\n        intervals = []\n        for start, end, word in utt:\n            intervals.append((start, end, word))\n        # print((k, intervals, 0, duration))\n        # print(f'------------- {name}: {duration}')\n        tg.addTier(textgrid.IntervalTier(k, intervals, 0, duration))\n    try:\n        tg.save(join(outputPath, name + \".TextGrid\"), format=\"short_textgrid\", includeBlankSpaces=False)\n    except:\n        print(outputPath, name)\n        print(text.split('\\n'))\n        print(f\"utt: {utt}\")\n        print(f\"voices: {voices}\")\n        print(f\"utterances: {utterances}\")\n        break\n\n# Did it work?\nfor fn in os.listdir(outputPath):\n    ext = os.path.splitext(fn)[1]\n    if ext != \".TextGrid\":\n        continue\n    # print(fn)\nprint('done')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3412/3412 [00:01<00:00, 2977.04it/s]\n\n\ndone\n\n\n\n\n\n\ninputPath = {'audios': f'{Path.home()}/.cache/panda/audios/', 'transcripts': f'{Path.home()}/.cache/panda/transcripts/'}\noutputPath = f'{Path.home()}/.cache/panda/txts/'\n\nif not os.path.exists(outputPath):\n    os.mkdir(outputPath)\n\nv_ix = 0\nfor fn in tqdm(os.listdir(inputPath['audios'])):\n    v_ix += 1\n    name, ext = os.path.splitext(fn)\n    if ext != \".wav\":\n        continue\n    with open(join(inputPath['transcripts'], f\"{name}.txt\"), 'r') as f:\n        text = f.read()\n        utterances = text.split('\\n')\n        utterances = [re.sub('\\[.+?\\]|-|\\.', '', u).strip() for u in utterances]\n    utterance = ' '.join(utterances)\n    with open(join(outputPath, name + \".txt\"), 'w') as f:\n        f.write(utterance)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3412/3412 [00:00<00:00, 9347.73it/s]\n\n\n\nfrom glob import glob\nfrom pydub import AudioSegment\n\naudio_output = f'{Path.home()}/.cache/panda/audio_slices'\ntrans_output = f'{Path.home()}/.cache/panda/trans_slices'\ngrids = glob('output/*.TextGrid')\nfor grid in grids:\n    name = Path(grid).stem\n    tg = textgrid.openTextgrid(f\"output/{name}.TextGrid\", includeEmptyIntervals=False)\n    audio = AudioSegment.from_wav(f\"{Path.home()}/.cache/panda/audios/{name}.wav\")\n    for ix, (k, v) in enumerate(tg.tierDict.items()):\n        if 'phones' in k:\n            continue\n        # print(f\"{k}-------------\")\n        for s, e, t in v.entryList:\n            start = max(s-0.1, 0)*1000\n            end = min(e+0.1, audio.duration_seconds)*1000\n            # print(f\"{t} => {s}:{e} => {e-s}\")\n            audio_slice = audio[start:end]\n            audio_slice.export(f'{audio_output}/{t}_{ix}_{name}.wav', format=\"wav\")\n            with open(f'{trans_output}/{t}_{ix}_{name}.txt', 'w') as f:\n                f.write(t)"
  },
  {
    "objectID": "data/panda/build_wav2vec2_embeddings.html",
    "href": "data/panda/build_wav2vec2_embeddings.html",
    "title": "wav2keyword",
    "section": "",
    "text": "from datasets import load_dataset, Features, Value, Audio, ClassLabel\nimport json\n\nfeats = Features({\"path\": Value(\"string\"),\n                  \"audio\": Audio(sampling_rate=16_000),\n                  \"label\": ClassLabel(names=[\"not found\",\"found\"])}\n                  )\ndef _generate_examples(example, tag):\n        example['label'] = 1 if example['label'] in tag else 0\n        example['audio'] = example['path']\n        return example\n\nwith open('tags_data.json', 'r') as f:\n    data = json.load(f)\n\ndata_files = {'train': 'dataset/slices_train.csv', 'test': 'dataset/slices_test.csv', 'val': 'dataset/slices_val.csv'}\ndataset = load_dataset(\"csv\", data_files=data_files)\ndataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\ntags_pool = [k for k, v in data.items() if 'chow mein' in v['tags']]\ndataset = dataset.map(_generate_examples, fn_kwargs={'tag': tags_pool}, features=feats)\ndataset = dataset.rename_column('path', 'file')\nid2label = {0: 'not found', 1: 'found'}\nlabel2id = {v: k for k, v in id2label.items()}\n\nUsing custom data configuration default-c58ed15a5d5a3dac\nReusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)"
  },
  {
    "objectID": "data/panda/build_wav2vec2_embeddings.html#similarity-measure",
    "href": "data/panda/build_wav2vec2_embeddings.html#similarity-measure",
    "title": "wav2keyword",
    "section": "Similarity measure",
    "text": "Similarity measure\n\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\nfrom datasets import load_dataset\nimport torch\n\nsampling_rate = dataset['train'].features[\"audio\"].sampling_rate\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 35716\n    })\n    test: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 4482\n    })\n    val: Dataset({\n        features: ['file', 'audio', 'label'],\n        num_rows: 4429\n    })\n})\n\n\n\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom pathlib import Path\n\n# audio file is decoded on the fly\nmax_duration = 1\nfor batch in tqdm(range(32, 35716, 32)):\n    inputs = feature_extractor(\n        [d[\"array\"] for d in dataset['train'][batch-32:batch][\"audio\"]], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True, max_duration=max_duration\n    )\n    files = [Path(d).name for d in dataset['train'][batch-32:batch]['file']]\n    trans = [name.split('_')[0] for name in files]\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state\n    embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n    for i in range(embeddings.shape[0]):\n        Path(f\"embeddings/{trans[i]}\").mkdir(parents=True, exist_ok=True)\n        name = files[i].replace('.wav', '.pt')\n        torch.save(embeddings[i], f'embeddings/{trans[i]}/{name}')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1116/1116 [50:45<00:00,  2.73s/it] \n\n\n\nfrom glob import glob\nimport torch\nfrom tqdm import tqdm\nfrom collections import Counter\n\nshapes = []\nfor file in tqdm(glob('embeddings_base/*/*.pt')):\n    shapes.append(torch.load(file).shape[0])\n\n\nimport numpy as np\n\nnp.quantile(shapes, np.arange(0,1,0.1))\n\narray([19. , 33. , 38. , 40.7, 42. , 43. , 45. , 46. , 47. , 48. ])\n\n\n\nmax(shapes)\n\n49\n\n\n\nprint(torch.load(file).shape)\ntorch.nn.functional.pad(torch.load(file), (0, 0, 0, 2), \"constant\", 0)\n\ntorch.Size([46, 768])\n\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
  },
  {
    "objectID": "data/panda/vectorx.html",
    "href": "data/panda/vectorx.html",
    "title": "wav2keyword",
    "section": "",
    "text": "feats = Features({\"path\": Value(\"string\"),\n                  \"audio\": Audio(sampling_rate=16_000),\n                  \"label\": ClassLabel(names=names)}\n                  )\n\ndef _generate_examples(example, label2id: dict = label2id):\n        label = Path(example['path']).name.split('_')[0]\n        example['label'] = label2id.get(label, label2id['unk'])\n        example['audio'] = example['path']\n        return example\n\nwith open('tags_data.json', 'r') as f:\n    data = json.load(f)\n\ndata_files = {'train': 'dataset/slices_train.csv', 'test': 'dataset/slices_test.csv', 'val': 'dataset/slices_val.csv'}\ndataset = load_dataset(\"csv\", data_files=data_files)\ndataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\n\n\ndataset = dataset.map(_generate_examples, features=feats)\ndataset = dataset.rename_column('path', 'file')\n\nUsing custom data configuration default-c58ed15a5d5a3dac\nReusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import AutoFeatureExtractor\nfrom transformers import Wav2Vec2ForXVector, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\n\nmodel_checkpoint = \"facebook/wav2vec2-base\"\nbatch_size = 32\nmax_duration = 1\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, \n        sampling_rate=feature_extractor.sampling_rate, \n        max_length=int(feature_extractor.sampling_rate * max_duration), \n        truncation=True, \n    )\n    return inputs\n\nencoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)\n\n/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  tensor = as_tensor(value)\n\n\n\n\n\n\n\n\n\nnum_labels = len(id2label)\n\nmodel = Wav2Vec2ForXVector.from_pretrained(\n    model_checkpoint, \n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label,\n)\n\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-xvector\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=35,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    push_to_hub=False,\n)\n\n/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\nSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForXVector: ['quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.weight', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.weight']\n- This IS expected if you are initializing Wav2Vec2ForXVector from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForXVector from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForXVector were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['tdnn.0.kernel.weight', 'tdnn.3.kernel.bias', 'feature_extractor.weight', 'objective.weight', 'feature_extractor.bias', 'tdnn.0.kernel.bias', 'tdnn.4.kernel.weight', 'tdnn.3.kernel.weight', 'classifier.weight', 'tdnn.1.kernel.bias', 'tdnn.1.kernel.weight', 'tdnn.2.kernel.weight', 'tdnn.4.kernel.bias', 'projector.bias', 'projector.weight', 'classifier.bias', 'tdnn.2.kernel.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmetric = load_metric(\"f1\", cache_dir='/home/jovyan/.cache/huggingface/metrics')\nmetric\n\nMetric(name: \"f1\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\nArgs:\n    predictions (`list` of `int`): Predicted labels.\n    references (`list` of `int`): Ground truth labels.\n    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n\n        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n        - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n        - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n        - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n        - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n    sample_weight (`list` of `float`): Sample weights Defaults to None.\n\nReturns:\n    f1 (`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\n\nExamples:\n\n    Example 1-A simple binary example\n        >>> f1_metric = datasets.load_metric(\"f1\")\n        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\n        >>> print(results)\n        {'f1': 0.5}\n\n    Example 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\n        >>> f1_metric = datasets.load_metric(\"f1\")\n        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\n        >>> print(round(results['f1'], 2))\n        0.67\n\n    Example 3-The same simple binary example as in Example 1, but with `sample_weight` included.\n        >>> f1_metric = datasets.load_metric(\"f1\")\n        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\n        >>> print(round(results['f1'], 2))\n        0.35\n\n    Example 4-A multiclass example, with different values for the `average` input.\n        >>> predictions = [0, 2, 1, 0, 0, 1]\n        >>> references = [0, 1, 2, 0, 1, 2]\n        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n        >>> print(round(results['f1'], 2))\n        0.27\n        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\n        >>> print(round(results['f1'], 2))\n        0.33\n        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n        >>> print(round(results['f1'], 2))\n        0.27\n        >>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\n        >>> print(results)\n        {'f1': array([0.8, 0. , 0. ])}\n\"\"\", stored examples: 0)\n\n\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    logits = eval_pred.predictions[0]\n    proj = model.objective._parameters['weight'].cpu().detach().numpy()\n    predicted_labels = np.argmax(np.dot(logits, proj), axis=1)\n    res = metric.compute(predictions=predicted_labels, references=eval_pred.label_ids, average='weighted')\n    print(res)\n    return res\n\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset[\"val\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics\n)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 36993\n  Num Epochs = 35\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 4\n  Total optimization steps = 10115\n\n\n\n\n    \n      \n      \n      [10115/10115 6:58:41, Epoch 34/35]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      F1\n    \n  \n  \n    \n      0\n      9.972000\n      9.282754\n      0.494746\n    \n    \n      1\n      7.730100\n      6.484269\n      0.689371\n    \n    \n      2\n      5.968000\n      4.822059\n      0.795504\n    \n    \n      3\n      5.182500\n      3.685891\n      0.832956\n    \n    \n      4\n      4.352900\n      3.038280\n      0.865561\n    \n    \n      5\n      3.857300\n      2.449143\n      0.896701\n    \n    \n      6\n      2.967500\n      2.071967\n      0.909700\n    \n    \n      7\n      3.039100\n      1.890948\n      0.914620\n    \n    \n      8\n      2.665400\n      1.711604\n      0.927954\n    \n    \n      9\n      2.360600\n      1.603896\n      0.932025\n    \n    \n      10\n      2.303300\n      1.562655\n      0.936823\n    \n    \n      11\n      2.261800\n      1.522344\n      0.936481\n    \n    \n      12\n      1.766600\n      1.525429\n      0.938809\n    \n    \n      13\n      1.663100\n      1.264167\n      0.948110\n    \n    \n      14\n      1.854700\n      1.188033\n      0.953547\n    \n    \n      15\n      1.729100\n      1.124112\n      0.956645\n    \n    \n      16\n      1.399600\n      1.002161\n      0.960203\n    \n    \n      17\n      1.354000\n      0.957133\n      0.960323\n    \n    \n      18\n      1.482800\n      0.940949\n      0.964454\n    \n    \n      19\n      1.262700\n      0.937568\n      0.964595\n    \n    \n      20\n      1.270600\n      0.899725\n      0.964520\n    \n    \n      21\n      1.222900\n      0.820089\n      0.969124\n    \n    \n      22\n      0.948800\n      0.863048\n      0.966118\n    \n    \n      23\n      0.975500\n      0.852564\n      0.966520\n    \n    \n      24\n      0.817600\n      0.781341\n      0.971060\n    \n    \n      25\n      0.927600\n      0.751091\n      0.971370\n    \n    \n      26\n      0.967400\n      0.775151\n      0.970824\n    \n    \n      27\n      0.888000\n      0.725526\n      0.974405\n    \n    \n      28\n      0.750600\n      0.725868\n      0.972901\n    \n    \n      29\n      0.806300\n      0.701442\n      0.972335\n    \n    \n      30\n      0.798500\n      0.698346\n      0.973509\n    \n    \n      31\n      0.663300\n      0.702570\n      0.974114\n    \n    \n      32\n      0.644100\n      0.699597\n      0.973188\n    \n    \n      33\n      0.736900\n      0.693265\n      0.973221\n    \n    \n      34\n      0.690700\n      0.686031\n      0.972811\n    \n  \n\n\n\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.49474644990778804}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-289\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-289/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-289/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-289/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.6893705687016398}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-578\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-578/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-578/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-578/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.7955035044782507}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-867\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-867/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-867/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-867/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.8329559775436511}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1156\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.865561052739612}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1445\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.8967009390545976}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1734\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9097000463410372}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2023\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9146199620187198}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2312\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9279538241747127}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2601\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9320251208426955}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2890\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9368229765369066}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3179\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9364810793693422}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3468\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9388092404231434}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3757\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9481100323068246}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4046\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9535473362353735}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4335\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9566449639826673}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4624\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9602031926260322}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4913\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9603232290773355}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5202\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9644543911689891}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5491\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9645954077460133}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5780\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9645197916695856}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6069\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9691239255185394}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6358\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9661176529231834}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6647\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9665197187638916}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6936\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9710602936387585}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7225\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9713701299682312}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7514\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9708238623533595}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7803\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9744048074888573}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8092\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.972901115518863}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8381\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9723351189232675}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8670\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9735086360418925}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8959\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9741139097461785}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9248\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9731876791817936}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9537\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9732213803231736}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9826\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 4586\n  Batch size = 32\n\n\n{'f1': 0.9728109377883826}\n\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-10115\nConfiguration saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from wav2vec2-base-finetuned-xvector/checkpoint-8092 (score: 0.9744048074888573).\n\n\nTrainOutput(global_step=10115, training_loss=2.4172881723452013, metrics={'train_runtime': 25124.955, 'train_samples_per_second': 51.533, 'train_steps_per_second': 0.403, 'total_flos': 1.2024582268257595e+19, 'train_loss': 2.4172881723452013, 'epoch': 35.0})\n\n\n\ntrainer.save_model(f\"{model_name}-finetuned-xvector/best_checkpoint\")\n\nSaving model checkpoint to wav2vec2-base-finetuned-xvector/best_checkpoint\nConfiguration saved in wav2vec2-base-finetuned-xvector/best_checkpoint/config.json\nModel weights saved in wav2vec2-base-finetuned-xvector/best_checkpoint/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-xvector/best_checkpoint/preprocessor_config.json\n\n\n\ntrainer._load_from_checkpoint(f\"{model_name}-finetuned-xvector/best_checkpoint\")\n\nLoading model from wav2vec2-base-finetuned-xvector/best_checkpoint.\n\n\n\ninputs = encoded_dataset['test']\n\nwith torch.no_grad():\n        result = trainer.predict(test_dataset = inputs)\nresult.metrics\n\n***** Running Prediction *****\n  Num examples = 4648\n  Batch size = 32\n\n\n\n\n\n{'f1': 0.9685338664124005}\n\n\n{'test_loss': 0.8347758054733276,\n 'test_f1': 0.9685338664124005,\n 'test_runtime': 37.9534,\n 'test_samples_per_second': 122.466,\n 'test_steps_per_second': 3.847}\n\n\n\nfrom collections import Counter\ndef get_predicted_labels(logits):\n    proj = model.objective._parameters['weight'].cpu().detach().numpy()\n    return np.argmax(np.dot(logits, proj), axis=1)\n    \npredicted_labels = get_predicted_labels(result.predictions[0])\nhits = [(t, p==t) for p, t in zip(predicted_labels, result.label_ids)]\n\nper_label_acc = {}\nfor l, h in hits:\n    if h:\n        per_label_acc[l] = per_label_acc.get(l, 0) + 1\naccs = []\nfor k, v in per_label_acc.items():\n    n = Counter(result.label_ids)[k]\n    accs.append((id2label[k], round(v/n, 2), n))\nsorted(accs, key=lambda x: x[2], reverse=True)\n\n[('chicken', 1.0, 1811),\n ('orange', 1.0, 346),\n ('rice', 0.9970149253731343, 335),\n ('entrees', 1.0, 204),\n ('entree', 1.0, 199),\n ('can', 0.9259259259259259, 162),\n ('honey', 1.0, 102),\n ('shrimp', 1.0, 98),\n ('steak', 1.0, 94),\n ('drinks', 1.0, 82),\n ('steamed', 1.0, 70),\n ('unk', 0.7246376811594203, 69),\n ('chow', 0.8382352941176471, 68),\n ('tea', 1.0, 67),\n ('drink', 1.0, 66),\n ('plate', 0.8032786885245902, 61),\n ('side', 1.0, 58),\n ('mein', 0.9814814814814815, 54),\n ('beef', 0.8958333333333334, 48),\n ('lemonade', 1.0, 47),\n ('bowl', 0.9333333333333333, 45),\n ('large', 1.0, 41),\n ('small', 0.9411764705882353, 34),\n ('one', 0.6451612903225806, 31),\n ('greens', 1.0, 28),\n ('two', 0.6538461538461539, 26),\n ('fried', 0.76, 25),\n ('medium', 1.0, 24),\n ('walnut', 0.9090909090909091, 22),\n ('mushroom', 0.9473684210526315, 19),\n ('teriyaki', 0.9444444444444444, 18),\n ('kung', 0.5, 16),\n ('broccoli', 1.0, 16),\n ('pao', 0.8571428571428571, 14),\n ('meal', 0.9285714285714286, 14),\n ('instead', 1.0, 14),\n ('pepper', 0.7142857142857143, 14),\n ('coke', 1.0, 13),\n ('strawberry', 1.0, 13),\n ('black', 0.9166666666666666, 12),\n ('chickens', 1.0, 11),\n ('bigger', 0.9, 10),\n ('bowls', 0.8, 10),\n ('green', 0.8888888888888888, 9),\n ('veggie', 1.0, 8),\n ('raspberry', 1.0, 8),\n ('beijing', 0.875, 8),\n ('breast', 0.625, 8),\n ('regular', 0.75, 8),\n ('milk', 0.625, 8),\n ('cheese', 0.5, 8),\n ('egg', 0.875, 8),\n ('grilled', 0.7142857142857143, 7),\n ('three', 0.5714285714285714, 7),\n ('plates', 0.8333333333333334, 6),\n ('super', 1.0, 6),\n ('angus', 1.0, 6),\n ('sides', 1.0, 6),\n ('white', 0.3333333333333333, 6),\n ('water', 1.0, 5),\n ('veggies', 1.0, 5),\n ('diet', 1.0, 3),\n ('rangoons', 0.6666666666666666, 3),\n ('vegetables', 1.0, 2),\n ('bottle', 1.0, 1)]\n\n\n\nfrom numpy import dot\nfrom numpy.linalg import norm\nfrom scipy import spatial\nfrom tqdm import tqdm\n\ncosine_sim = torch.nn.CosineSimilarity(dim=-1)\nsimilarities = []\nfor ix, emb in tqdm(enumerate(result.predictions[1]), total=len(result.predictions[1])):\n    max_sim = 0\n    max_label = None\n    for jx, emb_2 in enumerate(result.predictions[1]):\n        if ix == jx:\n            continue\n        # sim = dot(emb, emb_2)/(norm(emb)*norm(emb_2))\n        sim = 1 - spatial.distance.cosine(emb, emb_2)\n        if sim > max_sim:\n            max_sim = sim\n            max_label = id2label[result.label_ids[jx]]\n    l = id2label[result.label_ids[ix]]\n    similarities.append((l, max_label))\nsimilarities[:5]\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4648/4648 [14:14<00:00,  5.44it/s]\n\n\n[('water', 'water'),\n ('water', 'water'),\n ('water', 'water'),\n ('small', 'small'),\n ('small', 'small')]\n\n\n\nsum([t==p for t, p in similarities])/len(similarities)\n\n0.9677280550774526\n\n\n\nper_label_acc = {}\nfor t, p in similarities:\n    if t==p:\n        per_label_acc[t] = per_label_acc.get(t, 0) + 1\naccs = []\nfor k, v in per_label_acc.items():\n    n = Counter(result.label_ids)[label2id[k]]\n    accs.append((k, round(v/n, 2), n))\nsorted(accs, key=lambda x: x[2], reverse=True)\n\n[('chicken', 1.0, 1811),\n ('orange', 1.0, 346),\n ('rice', 0.99, 335),\n ('entrees', 1.0, 204),\n ('entree', 1.0, 199),\n ('can', 0.9, 162),\n ('honey', 1.0, 102),\n ('shrimp', 1.0, 98),\n ('steak', 1.0, 94),\n ('drinks', 1.0, 82),\n ('steamed', 1.0, 70),\n ('unk', 0.64, 69),\n ('chow', 0.9, 68),\n ('tea', 1.0, 67),\n ('drink', 1.0, 66),\n ('plate', 0.84, 61),\n ('side', 1.0, 58),\n ('mein', 0.94, 54),\n ('beef', 0.88, 48),\n ('lemonade', 1.0, 47),\n ('bowl', 0.93, 45),\n ('large', 1.0, 41),\n ('small', 0.94, 34),\n ('one', 0.61, 31),\n ('greens', 0.96, 28),\n ('two', 0.73, 26),\n ('fried', 0.8, 25),\n ('medium', 0.96, 24),\n ('walnut', 0.91, 22),\n ('mushroom', 0.89, 19),\n ('teriyaki', 0.94, 18),\n ('kung', 0.56, 16),\n ('broccoli', 1.0, 16),\n ('pao', 0.86, 14),\n ('meal', 0.93, 14),\n ('instead', 1.0, 14),\n ('pepper', 0.71, 14),\n ('coke', 1.0, 13),\n ('strawberry', 1.0, 13),\n ('black', 0.92, 12),\n ('chickens', 1.0, 11),\n ('bigger', 0.9, 10),\n ('bowls', 0.8, 10),\n ('green', 0.78, 9),\n ('veggie', 1.0, 8),\n ('raspberry', 1.0, 8),\n ('beijing', 0.88, 8),\n ('breast', 0.88, 8),\n ('regular', 0.88, 8),\n ('milk', 1.0, 8),\n ('cheese', 0.62, 8),\n ('egg', 0.88, 8),\n ('grilled', 0.71, 7),\n ('three', 0.57, 7),\n ('plates', 1.0, 6),\n ('super', 1.0, 6),\n ('angus', 1.0, 6),\n ('sides', 0.83, 6),\n ('white', 0.17, 6),\n ('water', 1.0, 5),\n ('veggies', 0.8, 5),\n ('diet', 1.0, 3),\n ('rangoons', 0.33, 3),\n ('vegetables', 1.0, 2)]"
  },
  {
    "objectID": "data/panda/build_Dataset.html",
    "href": "data/panda/build_Dataset.html",
    "title": "wav2keyword",
    "section": "",
    "text": "Import items tag data\n\nimport json\n\nwith open('pd-112705/12-tag.json', 'r') as f:\n    config = json.load(f)\ntags = [{'tag_class': t['tag_class'], 'tags': t['tags'], 'files': []} for t in config]\nprint(f\"items: {len(tags)}\")\n\nitems: 474\n\n\n\n\nGroup transcriptions and audio by items\n\nfrom collections import defaultdict\nimport re\nfrom random import uniform, seed\n\nseed(1991)\n\nfor tag in tags:\n    for k, v in audios_data.items():\n        for t in tag['tags']:\n            if t in v['transcript']:\n                tag['files'].append(k)\n\ndata = {}\nfor tag in tags:\n    t = tag['tag_class'].lower()\n    t = re.sub(r'\\bdefault\\b|\\btag class\\b|\\basr\\b|\\bfollowup\\b|\\broot\\b|\\bunavailable\\b|\\bcorrection\\b|\\bfollow-up\\b', '', t).strip()\n    if data.get(t):\n        data[t]['tags'].extend(tag['tags'])\n        data[t]['files'].extend(tag['files'])\n    else:\n        data[t] = {}\n        data[t]['tags'] = tag['tags']\n        data[t]['files'] = tag['files']\n\nfor k, v in data.items():\n    v['tags'] = list(set(v['tags']))\n    v['files'] = list(set(v['files']))\n    v['split'] = [uniform(0, 1) for _ in range(len(v['files']))]\n    v['split'] = ['train' if r < 0.8 else r for r in v['split']]\n    v['split'] = ['val' if (isinstance(r, float)) and (r < 0.9) else r for r in v['split']]\n    v['split'] = ['test' if isinstance(r, float) else r for r in v['split']]\n\nsorted([(k, len(v['files'])) for k, v in data.items()], key=lambda x: x[1], reverse=True)[:5]\n\n[('the original orange chicken', 2005),\n ('kung pao chicken', 1585),\n ('chow mein', 1504),\n ('grilled teriyaki chicken', 1487),\n ('mushroom chicken', 1385)]\n\n\n\nwith open('tags_data.json', 'w') as f:\n    json.dump(data, f)\n\n\ntags_pool = [k for k, v in data.items() if 'chow mein' in v['tags']]\ntags_pool\n\n['chow mein']\n\n\n\n\nAdd label\n\n\nMake a dataset for each item\n\nfrom pathlib import Path\nimport shutil\nimport pandas as pd\n\nrecords = []\nfor k, v in data.items():\n    current_records = []\n    for f, s in zip(v['files'], v['split']):\n        # curr_path = f\"{Path.home()}/.cache/panda/audio_slices/{f}.wav\"\n        new_path = f\"{Path.home()}/.cache/panda/audio_slices/{f}.wav\"\n        # shutil.copyfile(curr_path, new_path)\n        current_records.append((k, new_path, s))\n    records.extend(current_records)\n\ndf = pd.DataFrame.from_records(records, columns=['label', 'path', 'split'])\nPath(f\"dataset\").mkdir(parents=True, exist_ok=True)\ntrain = df.query(f\"split == 'train'\")\nval = df.query(f\"split == 'val'\")\ntest = df.query(f\"split == 'test'\")\ntrain.to_csv('dataset/slices_train.csv')\nval.to_csv('dataset/slices_val.csv')\ntest.to_csv('dataset/slices_test.csv')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      label\n      path\n      split\n    \n  \n  \n    \n      0\n      aquafina\n      /home/jovyan/.cache/panda/audio_slices/water_0...\n      train\n    \n    \n      1\n      aquafina\n      /home/jovyan/.cache/panda/audio_slices/vitamin...\n      train\n    \n    \n      2\n      aquafina\n      /home/jovyan/.cache/panda/audio_slices/water_0...\n      val\n    \n    \n      3\n      aquafina\n      /home/jovyan/.cache/panda/audio_slices/waters_...\n      train\n    \n    \n      4\n      aquafina\n      /home/jovyan/.cache/panda/audio_slices/water_0...\n      test"
  },
  {
    "objectID": "data/panda/siamese_wav2vec.html",
    "href": "data/panda/siamese_wav2vec.html",
    "title": "wav2keyword",
    "section": "",
    "text": "import logging\nfrom collections import OrderedDict\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import Trainer\nfrom torch import optim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n# -\n\nimport numpy as np\nnp.random.seed(0)\n\n/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\nfrom tqdm import tqdm\nfrom glob import glob\nfrom random import choices, seed\nseed(1991)\nN = 100000\n\nclass TableDistanceDataset(torch.utils.data.Dataset):\n    def __init__(self, embeddings_files, labels):\n        self.X1 = []\n        self.X2 = []\n        self.dist = []\n        files = {}\n        for e in embeddings_files:\n            word = e.split('/')[-2]\n            files[word] = files.get(word, []) + [e]\n        n_w = len(files.keys())\n        for _ in tqdm(range(N)):\n            c = choices(list(files.keys()), k=1)[0]\n            chosen_file_x1 = choices(files[c], k=1)[0]\n            weights = [0.5/(n_w-1) if k != c else 0.5 for k in files.keys()]\n            chosen_word = choices(list(files.keys()), k=1, weights=weights)[0]\n            chosen_file_x2 = choices(files[chosen_word], k=1)[0]\n            self.X1.append(chosen_file_x1)\n            self.X2.append(chosen_file_x2)\n            self.dist.append(0 if c == chosen_word else 1)\n        self.dist = torch.Tensor(self.dist)\n\n\n    def __len__(self):\n        return len(self.X1)\n\n    def __getitem__(self, index):\n        x1 = torch.load(self.X1[index])\n        x2 = torch.load(self.X2[index])\n        return F.pad(x1, (0, 0, 0, 49-x1.shape[0]), 'constant', 0)[None, :, :], F.pad(x2, (0, 0, 0, 49-x2.shape[0]), 'constant', 0)[None, :, :], self.dist[index]\n\nembeddings_files = choices(glob('embeddings_base/*/*.pt'), k=800)\nlabels = [e.split('/')[-2] for e in embeddings_files]\ntableDistanceDataset = TableDistanceDataset(embeddings_files, labels)\nprint(f\"{N} == {len(tableDistanceDataset)}\")\n\nBATCH_SIZE = 32\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:01<00:00, 50336.83it/s]\n\n\n100000 == 100000\n\n\n\n\n\n\nimport re\n\n\nixs = [ix for ix, f in enumerate(tableDistanceDataset.X1) if re.findall('beef_0_1655590825-SIP-A90CCE12F2CF-00003f00-chunk3', f)]\nfor i in ixs:\n    if 'drink' in tableDistanceDataset.X2[i]:\n        print(tableDistanceDataset.X1[i])\n        print(tableDistanceDataset.X2[i])\n        print(tableDistanceDataset.dist[i])\n        print()\n\nembeddings_base/beef/beef_0_1655590825-SIP-A90CCE12F2CF-00003f00-chunk3.pt\nembeddings_base/drink/drink_0_1655668162-SIP-A90CCE12F2CF-00004093-chunk7.pt\ntensor(1.)\n\n\n\n\nfrom collections import Counter\n\nin_vocab_words = [f.split('/')[-2] for f in embeddings_files]\nout_vocab_words = [f.split('/')[-2] for f in glob('embeddings_base/*/*.pt') if f.split('/')[-2] not in in_vocab_words]\nprint(f\"in_vocab_words: {len(set(in_vocab_words))} , out_vocab_words: {len(set(out_vocab_words))}\")\nprint(f\"10 in vocab with most audios per word {Counter(in_vocab_words).most_common(10)}\")\nprint(f\"10 out vocab with most audios per word {Counter(out_vocab_words).most_common(10)}\")\n\nin_vocab_words: 84 , out_vocab_words: 69\n10 in vocab with most audios per word [('chicken', 91), ('orange', 63), ('can', 63), ('rice', 56), ('mein', 45), ('chow', 37), ('plate', 37), ('beef', 25), ('two', 22), ('bowl', 20)]\n10 out vocab with most audios per word [('bowls', 22), ('sprite', 22), ('brown', 17), ('noodles', 16), ('fry', 15), ('rangoons', 13), ('iced', 9), ('juice', 8), ('come', 7), ('vegetables', 6)]\n\n\n\nclass Table2Representation(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.nhid = 32\n\n        # build model\n        self.__build_model()\n    \n    def __build_model(self):\n        self.fc1 = nn.Conv2d(1, 20, 20, stride=2)\n        self.do1 = nn.Dropout(0.2)\n        self.out = nn.Conv2d(20, 1, 14, stride=2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.do1(x)\n        x = self.out(x)\n        return x.squeeze()\n\n\n# Based upon https://github.com/PyTorchLightning/Siamese-Neural-Networks/blob/master/model.py\nclass TableDistanceModule(pl.LightningModule):\n    def __init__(self, tableDistanceDataset):\n        super().__init__()\n\n        self.tableDistanceDataset = tableDistanceDataset\n        self.datatrain, self.dataval, self.datatest = \\\n        torch.utils.data.random_split(self.tableDistanceDataset,\n                                      [round(N*0.8),\n                                       round(N*0.1),\n                                       round(N*0.1)])\n\n        self.table2Representation = Table2Representation()\n\n        # build model\n        self.__build_model()\n    \n    def __build_model(self):\n        pass\n\n    def forward(self, x1, x2):\n        z1 = self.table2Representation.forward(x1)\n        z2 = self.table2Representation.forward(x2)\n        dis = torch.mean(torch.abs(z1 - z2), axis=1)\n        return dis\n\n    def loss(self, pred_dists, true_dists):\n        loss_val = F.mse_loss(pred_dists, true_dists)\n        return loss_val\n    \n    def _step(self, batch, batch_idx, name, training_step=False):\n        X1, X2, dist = batch\n        pred = self.forward(X1, X2)\n        loss_val = self.loss(pred, dist)\n        tqdm_dict = OrderedDict({name: loss_val})\n        self.log_dict(tqdm_dict)\n        if training_step:\n            return OrderedDict({\n                'loss': loss_val,\n                'progress_bar': tqdm_dict,\n                'log': tqdm_dict\n            })\n        else:\n            return tqdm_dict\n        \n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, name=\"train_loss\", training_step=True)\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, name=\"val_loss\", training_step=False)\n    def test_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, name=\"test_loss\", training_step=False)\n\n    def _epoch_end(self, outputs, name):\n        # With DP training I think you have to average the things individually? Not sure\n        # Look at the pytorch lightning siamese network code\n        #if self.trainer.use_dp or self.trainer.use_ddp2:\n        #    val_acc = torch.mean(val_acc)\n        avg_loss = torch.stack([x[name] for x in outputs]).mean()\n        tqdm_dict = {name: avg_loss}\n        self.log_dict(tqdm_dict)\n        result = OrderedDict({name: avg_loss, 'progress_bar': tqdm_dict, 'log': tqdm_dict})\n        return result\n        \n    def validation_epoch_end(self, outputs):\n        result = self._epoch_end(outputs, name=\"val_loss\")\n        self.log_dict(result)\n        return result\n    def test_epoch_end(self, outputs):\n        result = self._epoch_end(outputs, name=\"test_loss\")\n        self.log_dict(result)\n        return result\n        \n    # ---------------------\n    # TRAINING SETUP\n    # ---------------------\n    def configure_optimizers(self):\n        \"\"\"\n        return whatever optimizers we want here\n        :return: list of optimizers\n        \"\"\"\n        optimizer = optim.SGD(self.parameters(),\n                             lr=0.01, momentum=0.90)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                         T_max=10)\n        return [optimizer], [scheduler]\n\n    def __dataloader(self, train, dataset):\n        # when using multi-node (ddp) we need to add the  datasampler\n        train_sampler = None\n        batch_size = BATCH_SIZE\n\n        should_shuffle = train and train_sampler is None\n        loader = DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=should_shuffle,\n            sampler=train_sampler,\n            num_workers=0,\n            drop_last=True\n        )\n\n        return loader\n\n    def train_dataloader(self):\n        logging.info('training data loader called')\n        return self.__dataloader(train=True, dataset=self.datatrain)\n\n    def val_dataloader(self):\n        logging.info('val data loader called')\n        return self.__dataloader(train=False, dataset=self.dataval)\n\n    def test_dataloader(self):\n        logging.info('val data loader called')\n        return self.__dataloader(train=False, dataset=self.datatest)\n\n\nmodel_gpu = TableDistanceModule(tableDistanceDataset)\ntrainer_gpu = Trainer(max_epochs=15, gpus=-1)\nfor i, (x, y, d) in enumerate(model_gpu.datatrain):\n    print(x.device)\n    print(y.device)\n    print(d.device)\n    break\nfor p in model_gpu.parameters():\n    print(p.device)\ntrainer_gpu.fit(model_gpu)\n\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n  rank_zero_deprecation(\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nMissing logger folder: /workspaces/wav2keyword/data/panda/lightning_logs\n\n\ncpu\ncpu\ncpu\ncpu\ncpu\ncpu\ncpu\n\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name                 | Type                 | Params\n--------------------------------------------------------------\n0 | table2Representation | Table2Representation | 11.9 K\n--------------------------------------------------------------\n11.9 K    Trainable params\n0         Non-trainable params\n11.9 K    Total params\n0.048     Total estimated model params size (MB)\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:288: UserWarning: The ``compute`` method of metric _ResultMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n  rank_zero_warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=15` reached.\n\n\n\ntrainer_gpu.test()\n\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1386: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n  rank_zero_warn(\nRestoring states from the checkpoint path at /workspaces/wav2keyword/data/panda/lightning_logs/version_0/checkpoints/epoch=14-step=37500.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoaded model weights from checkpoint at /workspaces/wav2keyword/data/panda/lightning_logs/version_0/checkpoints/epoch=14-step=37500.ckpt\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n       Test metric             DataLoader 0\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      log:test_loss         0.14371803402900696\n progress_bar:test_loss     0.14371803402900696\n        test_loss           0.14371803402900696\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n[{'test_loss': 0.14371803402900696,\n  'progress_bar': {'test_loss': tensor(0.1437, device='cuda:0')},\n  'log': {'test_loss': tensor(0.1437, device='cuda:0')}}]\n\n\n\npos_ratio_test = []\n\nfor i, (x, y, d) in tqdm(enumerate(model_gpu.datatest), total=len(model_gpu.datatest)):\n    pos_ratio_test.append(d.item())\n\nsum(pos_ratio_test)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:24<00:00, 408.57it/s]\n\n\n4999.0\n\n\n\nsum(pos_ratio_test)/len(pos_ratio_test)\n\n0.4999\n\n\n\nmodel = TableDistanceModule.load_from_checkpoint('lightning_logs/version_0/checkpoints/epoch=14-step=37500.ckpt', tableDistanceDataset=tableDistanceDataset)\nmodel_gpu = TableDistanceModule(tableDistanceDataset)\n\n\nimport time \n\n\n# disable randomness, dropout, etc...\nmodel.eval()\n\n# predict with the model\npos_diff = []\nneg_diff = []\npos_acc = []\nneg_acc = []\nfor ix, (x, y, d) in enumerate(iter(model_gpu.test_dataloader())):\n    non_trivial = [ix for ix, (a, b) in enumerate(zip(x, y)) if ~torch.equal(a, b)]\n    x = x[non_trivial]\n    y = y[non_trivial]\n    d_hat = model(x, y)\n    indices = torch.nonzero(d)\n    nonzero_diff = torch.sum(torch.abs(d_hat[indices] - d[indices]), dim=0).item()\n    zero_diff = torch.sum(torch.abs(d_hat[d == 0] - d[d == 0]), dim=0).item()\n    pos_acc.append((sum([(0 if i.item() < 0.6 else 1) == t for i, t in zip(d_hat[indices], d[indices])]).item(),\n                    len(d[indices])))\n    neg_acc.append((sum([(0 if i.item() < 0.6 else 1) == t for i, t in zip(d_hat[d == 0], d[d == 0])]).item(),\n                    len(d[d == 0])))\n    pos_diff.append((nonzero_diff, len(d[indices])))\n    neg_diff.append((zero_diff, len(d[d == 0])))\n    if ix % 10 == 0:\n        print(f\"------{ix}----------\")\n        print(f\"positive diff: {round(sum([s for s, _ in pos_diff]) / sum([n for _, n in pos_diff]), 2)}\")\n        print(f\"positive acc: {round(sum([s for s, _ in pos_acc])/sum([l for _, l in pos_acc]), 2)}\")\n        print(f\"Negative diff: {round(sum([s for s, _ in neg_diff]) / sum([n for _, n in neg_diff]), 2)}\")\n        print(f\"negative acc: {round(sum([s for s, _ in neg_acc])/sum([l for _, l in neg_acc]), 2)}\")\n        print(f\"Overall diff: {round(sum([s1 + s2 for (s1, _), (s2, _) in zip(pos_diff, neg_diff)]) / sum([n1+n2 for (_, n1), (_, n2) in zip(pos_diff, neg_diff)]), 2)}\")\n\n------0----------\npositive diff: 0.31\npositive acc: 0.78\nNegative diff: 0.45\nnegative acc: 0.64\nOverall diff: 0.37\n------10----------\npositive diff: 0.29\npositive acc: 0.85\nNegative diff: 0.34\nnegative acc: 0.75\nOverall diff: 0.31\n------20----------\npositive diff: 0.28\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.77\nOverall diff: 0.3\n------30----------\npositive diff: 0.28\npositive acc: 0.85\nNegative diff: 0.32\nnegative acc: 0.77\nOverall diff: 0.3\n------40----------\npositive diff: 0.28\npositive acc: 0.83\nNegative diff: 0.31\nnegative acc: 0.77\nOverall diff: 0.3\n------50----------\npositive diff: 0.28\npositive acc: 0.84\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------60----------\npositive diff: 0.28\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------70----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------80----------\npositive diff: 0.28\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------90----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------100----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.32\nnegative acc: 0.76\nOverall diff: 0.3\n------110----------\npositive diff: 0.28\npositive acc: 0.85\nNegative diff: 0.32\nnegative acc: 0.76\nOverall diff: 0.3\n------120----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.32\nnegative acc: 0.76\nOverall diff: 0.29\n------130----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.32\nnegative acc: 0.75\nOverall diff: 0.3\n------140----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------150----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------160----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------170----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------180----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------190----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------200----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------210----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------220----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------230----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------240----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------250----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------260----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------270----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------280----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------290----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------300----------\npositive diff: 0.27\npositive acc: 0.85\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n------310----------\npositive diff: 0.27\npositive acc: 0.86\nNegative diff: 0.31\nnegative acc: 0.76\nOverall diff: 0.29\n\n\n\ntest_indices = model_gpu.datatest.indices\nx1 = [x for ix, x in enumerate(model_gpu.datatest.dataset.X1) if ix in test_indices]\nx2 = [x for ix, x in enumerate(model_gpu.datatest.dataset.X2) if ix in test_indices]\ndists = [x for ix, x in enumerate(model_gpu.datatest.dataset.dist) if ix in test_indices]\n\n\nfrom pathlib import Path\n    \nimport shutil\npos_hits = 0\npos_errs = 0\npos_n = 0\nneg_hits = 0\nneg_errs = 0\nneg_n = 0\n\nfor ix, i in tqdm(enumerate(range(32, len(x1), 32)), total=int(len(x1)/32)):\n    x, y, label = x1[i-32:i], x2[i-32:i], dists[i-32:i]\n    ex = [torch.load(e) for e in x]\n    ey = [torch.load(e) for e in y]\n    ex= [F.pad(e[None], (0, 0, 0, 49-e.shape[0]), 'constant', 0) for e in ex]\n    ey= [F.pad(e[None], (0, 0, 0, 49-e.shape[0]), 'constant', 0) for e in ey]\n    ex = torch.stack(ex)\n    ey = torch.stack(ey)\n    pred = model(ex, ey)\n    for jx, (t, d, xf, yf) in enumerate(zip(label, pred, x, y)):\n        d = 0 if d < 0.6 else 1\n        neg_n += 1 if t == 0 else 0\n        neg_hits += 1 if (t == 0) and (d == 0) else 0\n        neg_errs += 1 if (t == 0) and (d == 1) else 0\n        pos_n += 1 if t == 1 else 0\n        pos_hits += 1 if (t == 1) and (d == 1) else 0\n        pos_errs += 1 if (t == 1) and (d == 0) else 0\n\n        if xf == yf:\n            continue\n        xn = Path(xf).name\n        yn = Path(yf).name\n        xtrans = xn.split('_')[0]\n        ytrans = yn.split('_')[0]\n        if (xtrans != ytrans) and (t == 0):\n            print(xn, yn, xtrans, ytrans, t, d)\n            break\n        if (xtrans == ytrans) and (t == 1):\n            print(xn, yn, xtrans, ytrans, t, d)\n            break\n        if d == 0 and t == 0:\n            Path(f\"results/emb_sim/correct_same/{xtrans}\").mkdir(parents=True, exist_ok=True)\n            shutil.copy(f\"{Path.home()}/.cache/panda/audio_slices/{yn.replace('.pt', '.wav')}\", f\"results/emb_sim/correct_same/{xtrans}/{ix}_{jx}_{yn.replace('.pt', '.wav')}\")\n        if d == 1 and t == 0:\n            Path(f\"results/emb_sim/incorrect_dif/{xtrans}\").mkdir(parents=True, exist_ok=True)\n            shutil.copy(f\"{Path.home()}/.cache/panda/audio_slices/{yn.replace('.pt', '.wav')}\", f\"results/emb_sim/incorrect_dif/{xtrans}/{ix}_{jx}_{yn.replace('.pt', '.wav')}\")\n        if d == 0 and t == 1:\n            Path(f\"results/emb_sim/incorrect_same/{xtrans}\").mkdir(parents=True, exist_ok=True)\n            shutil.copy(f\"{Path.home()}/.cache/panda/audio_slices/{yn.replace('.pt', '.wav')}\", f\"results/emb_sim/incorrect_same/{xtrans}/{ix}_{jx}_{yn.replace('.pt', '.wav')}\")\n        if d == 1 and t == 1:\n            Path(f\"results/emb_sim/correct_dif/{xtrans}\").mkdir(parents=True, exist_ok=True)\n            shutil.copy(f\"{Path.home()}/.cache/panda/audio_slices/{yn.replace('.pt', '.wav')}\", f\"results/emb_sim/correct_dif/{xtrans}/{ix}_{jx}_{yn.replace('.pt', '.wav')}\")\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312/312 [03:30<00:00,  1.48it/s]\n\n\n\nprint(pos_n)\nprint(neg_n)\n\n5005\n4979\n\n\n\nprint(f\"{round(pos_hits/pos_n, 2)*100}\")\nprint(f\"{round(neg_hits/neg_n, 2)*100}\")\n\n86.0\n76.0\n\n\n\nfor ix, i in tqdm(enumerate(range(32, len(x1), 32))):\n    x, y, label = x1[i-32:i], x2[i-32:i], dists[i-32:i]\n    transx = [Path(j).name.split('_')[0] for j in x]\n    transy = [Path(j).name.split('_')[0] for j in y]\n    for jx, jy, l in zip(transx, transy, label):\n        if (jx != jy) and l == 0:\n            print(jx, jy, l)\n        if (jx == jy) and l == 1:\n            print(jx, jy, l)\n\nprint(jx, jy, l)\n\n312it [00:00, 2232.65it/s]\n\n\ngreen green tensor(0.)"
  },
  {
    "objectID": "preprocesses.html",
    "href": "preprocesses.html",
    "title": "preprocesses",
    "section": "",
    "text": "Code\n\nsource\n\nPreprocessor\n\n Preprocessor (model_checkpoint:str='facebook/wav2vec2-base')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPreprocessor.preprocess\n\n Preprocessor.preprocess (dataset:datasets.dataset_dict.DatasetDict,\n                          fn_kwargs:dict=None)\n\n\n\n\nExamples\nBefore we can feed audio clips to our model, we need to preprocess them. This is done by ðŸ¤— Transformers FeatureExtractor which will normalize the inputs and put them in a format the model expects, as well as generate the other inputs that the model requires.\nWe wrote a function that will preprocess our samples. The _preprocess_function is an internal function that will instantiate our feature extractor with the AutoFeatureExtractor.from_pretrained method, which will ensure that we get a preprocessor that corresponds to the model architecture we want to use.\nThe argument truncation=True and the maximum sample length we will ensure that very long inputs like the ones in the _silence_ class can be safely batched.\n\n# data = dataloader_pipeline({'path': \"superb\", 'name': \"ks\"})\n# dataset = data['dataset']\nsample = dataset['train'][:5]\npreprocessor = Preprocessor()\npreprocessed_sample = preprocessor._preprocess_function(sample)\n\n\nsr = sample['audio'][0]['sampling_rate']\nprint(f\"Raw sample durations in seconds: {[a['array'].shape[0]/sr for a in sample['audio']]}\")\nprint(f\"preprocessed sample durations in seconds: {[a.shape[0]/sr for a in preprocessed_sample['input_values']]}\")\n\nRaw sample durations in seconds: [95.183125, 61.8056875, 61.253875, 60.0, 61.1555]\npreprocessed sample durations in seconds: [1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nTo apply this function on all utterances in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command.\nThis whole process is mapped in the preprocess function. The results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). ðŸ¤— Datasets warns you when it uses cached files, you can pass load_from_cache_file=False in the call to map to not use the cached files and force the preprocessing to be applied again.\n\npreprocess_dataset = preprocessor.preprocess(dataset)\npreprocess_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_values', 'label'],\n        num_rows: 51094\n    })\n    validation: Dataset({\n        features: ['input_values', 'label'],\n        num_rows: 6798\n    })\n    test: Dataset({\n        features: ['input_values', 'label'],\n        num_rows: 3081\n    })\n})\n\n\n\ndataset.cleanup_cache_files()\n\n{'train': 64, 'validation': 0, 'test': 0}"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "wav2vec",
    "section": "",
    "text": "source\n\nfoo\n\n foo (bar:str, foobar:str, extra:str='c')\n\nfoobars the bar.\nArgs: bar (str): the bar. foobar (str): the foobar. extra (str): extra bar.\nReturns: str: the foobar.\n\nLorem ipsum\n\nfoo('a', 'b')\n\n'foobar'\n\n\n\ndisplay(SVG('<svg height=\"100\" xmlns=\"http://www.w3.org/2000/svg\"><circle cx=\"50\" cy=\"50\" r=\"40\"/></svg>'))\n\n\n\n\n\nassert foo('a', 'b') == 'foobar'\n\n\nfoo('dont', 'execute')"
  },
  {
    "objectID": "07_audio_processor.html",
    "href": "07_audio_processor.html",
    "title": "wav2keyword",
    "section": "",
    "text": "import pandas as pd\n\ntest = pd.read_csv('data/panda/dataset/test.csv', index_col=0)\n\nfor group_name, df_group in test.groupby('label'):\n    output_path = os.path.join('chunks', group_name)\n    audios = df_group.path.values\n    chunk_selected_audios(audios, output_path)\n    break\n\n14346.99s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n14347.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n14347.01s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n14347.02s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\n\nfrom typing import List\nfrom collections import Counter\nfrom transformers import AutoFeatureExtractor\nfrom transformers import Wav2Vec2ForXVector, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\nimport pandas as pd\nfrom pathlib import Path\nimport torch\nfrom numpy.typing import NDArray\nfrom pydantic import BaseModel\nfrom io import BytesIO\n\nclass AudioArray(BaseModel):\n    array: List[float]\n\nclass XvectorInput(BaseModel):\n    model_input: List[List[float]]\n\nclass XvectorModel(object):\n\n    def __init__(self, model_checkpoint: str) -> None:\n        self.model_checkpoint = model_checkpoint\n        self.model = Wav2Vec2ForXVector.from_pretrained(self.model_checkpoint)\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(self.model_checkpoint)\n    \n    def preprocess_function(self, audio_arrays: List[AudioArray], max_duration: float = 1.0)\\\n         -> List[List[float]]:\n        arrays = [a.array for a in audio_arrays]\n        inputs = self.feature_extractor(\n            arrays,\n            sampling_rate=self.feature_extractor.sampling_rate, \n            max_length=int(self.feature_extractor.sampling_rate * max_duration),\n            truncation=True\n            )\n        return inputs\n    \n    def parse_audio(self, raw_data: bytes, sample_width: int, channels: int, frame_rate: int) -> List[AudioArray]:\n        audio = AudioSegment.from_raw(\n            BytesIO(raw_data), \n            sample_width=sample_width,\n            channels=channels,\n            frame_rate=frame_rate\n        )\n        result = []\n        for ix, segment in enumerate(chunk_audio(audio)):\n            result.append(segment)\n        return result\n\n    def encode_data(self, audio_arrays: List[AudioArray]) -> XvectorInput:\n        result = {'input_values': self.preprocess_function(audio_arrays)}\n        return XvectorInput.parse_obj(result)\n    \n    def get_logits(self, inputs: XvectorInput):\n        with torch.no_grad():\n            result = self.model(**inputs.dict())\n        return result\n    \n    def get_predicted_labels(self, logits):\n        proj = self.model.objective._parameters['weight'].cpu().detach().numpy()\n        return np.argmax(np.dot(logits, proj), axis=1)\n\nmodel_checkpoint = 'data/panda/wav2vec2-base-finetuned-xvector/best_checkpoint/'\n# xvector_model = XvectorModel(model_checkpoint)\n\nfile = '/home/jovyan/.cache/panda/audios/1655690608-SIP-A90CCE12F2CF-000041c0-chunk4.wav_3.wav'\nwith open(file, \"rb\") as f:\n    audio_bytes = bytearray()\n    while (byte := f.read(1)):\n        audio_bytes.append(byte)\n    xvector_model.parse_audio(audio_bytes, 2, 1, 16000)\n\nSyntaxError: invalid syntax (3449389385.py, line 69)\n\n\n\nUnion[Dict]\n\n\ndf_group.shape\n\n(336, 3)\n\n\n\naudios_path = f'{Path.home()}/.cache/panda/audios'\naudios = glob(f'{audios_path}/*.wav')\nchunk_selected_audios(audios[:5], 'test/')\n\n'sadadsa/sdad/'"
  },
  {
    "objectID": "05_one_vs_rest.html",
    "href": "05_one_vs_rest.html",
    "title": "Wav2vec2 one vs rest classification",
    "section": "",
    "text": "We already trained and run a model. The results over the test set are in the one_vs_rest folder. We tested it on chow mein.\nA label of 1 meant that the audio said chow mein, chow, or mein. A 0 meant that the audio said anything else.\nIn the found folder we saved the audios the model identified as being chow mein. In not found we saved the audios the model identified as not being chow mein.\nWe didnt filtered by if the prediction was either correct or incorrect.\n\nchow_mein_correct_files = glob('data/panda/results/one_vs_rest/chow mein/found/*.wav')\nchow_mein_incorrect_files = glob('data/panda/results/one_vs_rest/chow mein/not found/*.wav')\nprint(f\"Found audios: {len(chow_mein_correct_files)}\")\nprint(f\"Missed audios: {len(chow_mein_incorrect_files)}\")\n\nFound audios: 100\nMissed audios: 31\n\n\nWe will show how we constructed the albels for the dataset we used for training, validation and testing.\nlabel has the item tag corresponding to the audio, path has the audio path.\nThe item tag was selected by checking if the audio transcription corresponded to an item tag.\n\ndata_files = {'train': 'data/panda/dataset/slices_train.csv', 'test': 'data/panda/dataset/slices_test.csv', 'val': 'data/panda/dataset/slices_val.csv'}\ndataset = load_dataset(\"csv\", data_files=data_files)\ndataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\n\nwith open('data/panda/tags_data.json', 'r') as f:\n    tags_data = json.load(f)\ndataset\n\nUsing custom data configuration default-c58ed15a5d5a3dac\nReusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'path'],\n        num_rows: 36993\n    })\n    test: Dataset({\n        features: ['label', 'path'],\n        num_rows: 4648\n    })\n    val: Dataset({\n        features: ['label', 'path'],\n        num_rows: 4586\n    })\n})\n\n\nWe selected all the items that have chow mein as in its tags. In this case, only one item has it, which is called chow mein.\nWe build the binary label by setting 1 if the label corresponds to chow mein, 0 otherwise.\nSince we dont want to train another model, but only show the model performance, we will filter out all the examples that are not chow mein from the datasets.\n\ntags_pool = [k for k, v in tags_data.items() if 'chow mein' in v['tags']]\nprint(f\"tags: {tags_pool}\")\nchow_dataset = dataset.filter(lambda x: x['label'] in tags_pool)\nchow_dataset\n\ntags: ['chow mein']\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'path'],\n        num_rows: 1206\n    })\n    test: Dataset({\n        features: ['label', 'path'],\n        num_rows: 135\n    })\n    val: Dataset({\n        features: ['label', 'path'],\n        num_rows: 163\n    })\n})\n\n\nWe can now compute the model accuracy:\n\nprint(f\"Positive rate train: {round(1206/36993, 2)}\")\nprint(f\"Positive rate test: {round(135/4648, 2)}\")\nprint(f\"Positive rate val: {round(163/4586, 2)}\")\nprint(f\"Accuracy test: {round(len(chow_mein_correct_files)/135, 2)}\")\n\nPositive rate train: 0.03\nPositive rate test: 0.03\nPositive rate val: 0.04\nAccuracy test: 0.74\n\n\nThe transcription is contained in the file name, here we print the transcriptions that are in the found and not found folders.\nWe confirm the model couldnt learn to classify audios saying noodles as part of the chow mein item, even though noodles is part of the chow mein tags set and thus labeled as 1.\n\nfrom pathlib import Path\n\ntrans = []\nfor f in chow_mein_correct_files:\n    trans.append(Path(f).name.split('_')[0])\nprint(f\"Transcriptions in found correct audios: {set(trans)}\")\n\nTranscriptions in found correct audios: {'chow', 'side', 'mein'}\n\n\n\nfrom pathlib import Path\n\ntrans = []\nfor f in chow_mein_incorrect_files:\n    trans.append(Path(f).name.split('_')[0])\nprint(f\"Transcriptions in not found audios: {set(trans)}\")\n\nTranscriptions in not found audios: {'noodles', 'chow', 'side', 'mein'}\n\n\n\nSample of correct examples found\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(chow_mein_correct_files)-1)\n    example = chow_mein_correct_files[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nSample of incorrect examples found\n\nimport random\nimport numpy\nfrom IPython.display import display, Audio\nfrom scipy.io.wavfile import read\n\nfor _ in range(10):\n    rand_idx = random.randint(0, len(chow_mein_incorrect_files)-1)\n    example = chow_mein_correct_files[rand_idx]\n    display(Audio(data=example))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "training",
    "section": "",
    "text": "Code\n\nsource\n\nW2KTrainer\n\n W2KTrainer (model_checkpoint:str='facebook/wav2vec2-base',\n             metric:str='accuracy')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nW2KTrainer.get_training_args\n\n W2KTrainer.get_training_args (training_kwargs=None)\n\n\nsource\n\n\nW2KTrainer.build_trainer\n\n W2KTrainer.build_trainer (dataset, id2label, label2id, args=None,\n                           preprocess_kwargs:dict={'max_duration': 1.0})\n\n\n\n\nExamples\nFirst we load the data\n\ndata = dataloader_pipeline({'path': \"superb\", 'name': \"ks\"})\ndataset = data['dataset']\n\nReusing dataset superb (/home/jovyan/.cache/huggingface/datasets/superb/ks/1.9.0/ce836692657f82230c16b3bbcb93eaacdbfd7de4def3be90016f112d68683481)\n\n\n\n\n\nWe can download the pretrained model and fine-tune it. W2KTrainer will use the AutoModelForAudioClassification class. Like with the feature extractor, the from_pretrained method in W2KTrainer will download and cache the model for us. As the label ids and the number of labels are dataset dependent, we pass num_labels, label2id, and id2label alongside the dataset here.\nTo instantiate a Trainer, we will need to define the TrainingArguments, which is a class that contains all the attributes to customize the training. W2KTrainer has a default TrainingArguments setup, but you can override any number of those parameters by passing a dictionary with them over the args argument.\nSince we are using the default TrainingArguments, we are not passing any custom args.\n\nw2ktrainer = W2KTrainer()\ntrainer = w2ktrainer.build_trainer(dataset, data['id2label'], data['label2id'])\n\nloading metric\n\n\n  warnings.warn(\n\n\n\n\n\n  tensor = as_tensor(value)\n\n\n\n\n\n\n\n\n  warnings.warn(\nSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias']\n- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.weight', 'projector.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nThe warning is telling us we are throwing away some weights (the quantizer and project_q layers) and randomly initializing some other (the projector and classifier layers). This is expected in this case, because we are removing the head used to pretrain the model on an unsupervised Vector Quantization objective and replacing it with a new head for which we donâ€™t have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\nWe will review the default TrainingArguments before continuing. We set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size of 32 and set the number of epochs for training in 5, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the Trainer to load the best model it saved (according to metric_name) at the end of training.\n\nw2ktrainer.training_args\n\n{'evaluation_strategy': 'epoch',\n 'save_strategy': 'epoch',\n 'learning_rate': 3e-05,\n 'per_device_train_batch_size': 32,\n 'gradient_accumulation_steps': 4,\n 'per_device_eval_batch_size': 32,\n 'num_train_epochs': 5,\n 'warmup_ratio': 0.1,\n 'logging_steps': 10,\n 'load_best_model_at_end': True,\n 'metric_for_best_model': 'accuracy',\n 'push_to_hub': False,\n 'output_dir': 'wav2vec2-base-finetuned-ks'}\n\n\nW2KTrainer defines an internal method _compute_metrics for how to compute the metrics from the predictions, which will just use the metric passed during instantiation which defaults to accuracy.\nThe only preprocessing it has to do is to take the argmax of our predicted logits. This is all done internally to instantiate the Trainer, but we show it here for completeness:\n\nw2ktrainer.metric\n\nMetric(name: \"accuracy\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\nArgs:\n    predictions: Predicted labels, as returned by a model.\n    references: Ground truth labels.\n    normalize: If False, return the number of correctly classified samples.\n        Otherwise, return the fraction of correctly classified samples.\n    sample_weight: Sample weights.\nReturns:\n    accuracy: Accuracy score.\nExamples:\n\n    >>> accuracy_metric = datasets.load_metric(\"accuracy\")\n    >>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n    >>> print(results)\n    {'accuracy': 1.0}\n\"\"\", stored examples: 0)\n\n\n\n\nW2KTrainer._compute_metrics\n\n W2KTrainer._compute_metrics (eval_pred)\n\nComputes accuracy on a batch of predictions\nNow we can finetune our model by calling the train method:\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 51094\n  Num Epochs = 5\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1995\n\n\n\n\n    \n      \n      \n      [1995/1995 1:20:59, Epoch 4/5]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Accuracy\n    \n  \n  \n    \n      0\n      0.659700\n      0.567452\n      0.953074\n    \n    \n      1\n      0.292000\n      0.175072\n      0.976317\n    \n    \n      2\n      0.188100\n      0.116128\n      0.980141\n    \n    \n      3\n      0.176100\n      0.094171\n      0.979847\n    \n    \n      4\n      0.132100\n      0.090120\n      0.981906\n    \n  \n\n\n\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-399\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-399/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-399/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-399/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-798\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-798/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-798/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-798/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1197\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1197/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1197/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1197/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1596\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1596/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1596/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1596/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\nSaving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1995\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-1995/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-1995/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1995/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from wav2vec2-base-finetuned-ks/checkpoint-1995 (score: 0.9819064430714917).\n\n\nTrainOutput(global_step=1995, training_loss=0.4566893815097952, metrics={'train_runtime': 4864.0932, 'train_samples_per_second': 52.522, 'train_steps_per_second': 0.41, 'total_flos': 2.31918157475328e+18, 'train_loss': 0.4566893815097952, 'epoch': 5.0})\n\n\nWe can check with the evaluate method that our Trainer did reload the best model properly (if it was not the last one):\n\ntrainer.evaluate()\n\n***** Running Evaluation *****\n  Num examples = 6798\n  Batch size = 32\n\n\n\n    \n      \n      \n      [213/213 01:02]\n    \n    \n\n\n{'eval_loss': 0.09011975675821304,\n 'eval_accuracy': 0.9819064430714917,\n 'eval_runtime': 62.8135,\n 'eval_samples_per_second': 108.225,\n 'eval_steps_per_second': 3.391,\n 'epoch': 5.0}\n\n\nNow we export the best checkpoint.\n\ntrainer.save_model(f\"{w2ktrainer.training_args['output_dir']}/best_checkpoint\")\n\nSaving model checkpoint to wav2vec2-base-finetuned-ks/best_checkpoint\nConfiguration saved in wav2vec2-base-finetuned-ks/best_checkpoint/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/best_checkpoint/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/best_checkpoint/preprocessor_config.json"
  }
]