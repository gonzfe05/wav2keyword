{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c58ed15a5d5a3dac\n",
      "Reusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f328885d314a9da1f1d57c8d1063ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eba4228f00941458828f5ebbcbfa86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36993 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84ec06c2e1c4924baacfa7bb28b1467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4648 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef17836148a4809b181cd6fb40135b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4586 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from datasets import load_dataset, Features, Value, Audio, ClassLabel\n",
    "import json\n",
    "\n",
    "feats = Features({\"path\": Value(\"string\"),\n",
    "                  \"audio\": Audio(sampling_rate=16_000),\n",
    "                  \"label\": ClassLabel(names=[\"not found\",\"found\"])}\n",
    "                  )\n",
    "def _generate_examples(example, tag):\n",
    "        example['label'] = 1 if example['label'] in tag else 0\n",
    "        example['audio'] = example['path']\n",
    "        return example\n",
    "\n",
    "with open('tags_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_files = {'train': 'dataset/slices_train.csv', 'test': 'dataset/slices_test.csv', 'val': 'dataset/slices_val.csv'}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\n",
    "tags_pool = [k for k, v in data.items() if 'chow mein' in v['tags']]\n",
    "dataset = dataset.map(_generate_examples, fn_kwargs={'tag': tags_pool}, features=feats)\n",
    "dataset = dataset.rename_column('path', 'file')\n",
    "id2label = {0: 'not found', 1: 'found'}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/jovyan/.cache/panda/audio_slices/water_0_1655689126-SIP-A90CCE12F2CF-000041b2-chunk3.wav',\n",
       " 'audio': {'path': '/home/jovyan/.cache/panda/audio_slices/water_0_1655689126-SIP-A90CCE12F2CF-000041b2-chunk3.wav',\n",
       "  'array': array([ 6.1035156e-05,  3.3569336e-04, -4.2724609e-04, ...,\n",
       "         -4.8522949e-03,  1.3031006e-02,  2.9037476e-02], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    35787\n",
       "1     1206\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "dataset['train'].to_pandas().label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9723a7e77bb41d7b4d1d40ead05d8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98ed39fffcd4f91988a544993518d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f45fc7b1682415aa8a219fc34210832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "def _filter_by_duration(example, duration):\n",
    "    return len(example['audio']['array']) < duration * example['audio']['sampling_rate']\n",
    "\n",
    "dataset = dataset.filter(_filter_by_duration, fn_kwargs={'duration': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.22027972027972\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "class_counts = dataset['train'].to_pandas().label.value_counts()\n",
    "weight_positive_class = class_counts.iloc[0]/class_counts.iloc[1]\n",
    "print(weight_positive_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/preprocessor_config.json from cache at /home/jovyan/.cache/huggingface/transformers/d4583dd9e59eb6295f8fe8b18833ae54d963a122d69aa1df7ecce6caafe18c8f.bc3155ca0bae3a39fc37fc6d64829c6a765f46480894658bb21c08db6155358d\n",
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c1ae0720c2420ea677388d576048cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a81a7a5f96240ba9a9a18cd907b43e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ec551ce1504cd4843f0b6a2591ec6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"not found\",\n",
      "    \"1\": \"found\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"found\": 1,\n",
      "    \"not found\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-base/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'projector.bias', 'classifier.weight', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "model_checkpoint = \"facebook/wav2vec2-base\"\n",
    "batch_size = 32\n",
    "max_duration = 1\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=feature_extractor.sampling_rate, \n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration), \n",
    "        truncation=True, \n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ks\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=15,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (2 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.35]).cuda())\n",
    "        logits_view = logits.view(-1, self.model.config.num_labels)\n",
    "        loss = loss_fct(logits_view, labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"val\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35708\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 4185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4185' max='4185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4185/4185 2:23:46, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.306926</td>\n",
       "      <td>0.882207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.227008</td>\n",
       "      <td>0.904730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.193498</td>\n",
       "      <td>0.914414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.166870</td>\n",
       "      <td>0.923874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.159021</td>\n",
       "      <td>0.926802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.158407</td>\n",
       "      <td>0.931081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.141965</td>\n",
       "      <td>0.934685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.148205</td>\n",
       "      <td>0.929054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.138138</td>\n",
       "      <td>0.936036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.141322</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.133537</td>\n",
       "      <td>0.935811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>0.934910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.133605</td>\n",
       "      <td>0.933784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.132347</td>\n",
       "      <td>0.935811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.129441</td>\n",
       "      <td>0.936486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-279\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-279/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-279/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-279/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-558\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-558/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-558/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-558/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-837\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-837/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-837/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-837/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1116\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1116/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1116/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1116/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1395\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1395/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1395/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1395/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1674\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1674/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1674/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1674/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1953\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1953/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1953/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1953/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2232\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-2232/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-2232/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2232/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2511\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-2511/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-2511/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2511/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-2790\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-2790/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-2790/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-2790/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3069\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-3069/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-3069/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3069/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3348\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-3348/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-3348/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3348/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3627\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-3627/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-3627/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3627/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-3906\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-3906/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-3906/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-3906/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4440\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-4185\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-4185/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-4185/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-4185/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from wav2vec2-base-finetuned-ks/checkpoint-4185 (score: 0.9364864864864865).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4185, training_loss=0.21789302935594584, metrics={'train_runtime': 8628.3807, 'train_samples_per_second': 62.077, 'train_steps_per_second': 0.485, 'total_flos': 4.3021878417689375e+18, 'train_loss': 0.21789302935594584, 'epoch': 15.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 5, 'test': 1, 'val': 0, 'validation': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "dataset.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/best_checkpoint\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/best_checkpoint/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/best_checkpoint/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/best_checkpoint/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.save_model(f\"wav2vec2-base-finetuned-ks/best_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4479\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 1.2470611, -1.2712642],\n",
       "       [ 3.5061896, -3.6580167],\n",
       "       [ 3.2142575, -3.3506398],\n",
       "       ...,\n",
       "       [ 3.4247284, -3.5656137],\n",
       "       [ 3.067631 , -3.2232652],\n",
       "       [ 3.5443592, -3.709301 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.1558578908443451, 'test_accuracy': 0.9278856887698147, 'test_runtime': 28.6291, 'test_samples_per_second': 156.449, 'test_steps_per_second': 4.89})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "inputs = encoded_dataset['test']\n",
    "\n",
    "with torch.no_grad():\n",
    "        result = trainer.predict(test_dataset = inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chow mein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.9752176825184193\n",
      "Positive accutacy: 0.7633587786259542\n",
      "Negative accuracy: 0.9816007359705612\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "predicted_labels = np.argmax(result.predictions, axis=-1)\n",
    "positive_hits = [p==t for p, t in zip(predicted_labels, inputs['label']) if t == 1]\n",
    "correct_positive = sum(positive_hits)\n",
    "correct_negative = sum([p==t for p, t in zip(predicted_labels, inputs['label']) if t == 0])\n",
    "n_positive = sum(inputs['label'])\n",
    "n_negative = len(inputs['label']) - n_positive\n",
    "\n",
    "print(f\"Overall accuracy: {(correct_positive + correct_negative) / (n_positive + n_negative)}\")\n",
    "print(f\"Positive accutacy: {correct_positive / n_positive}\")\n",
    "print(f\"Negative accuracy: {correct_negative / n_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "positive_hits_files = [f for p, t, f in zip(predicted_labels, dataset['test']['label'], dataset['test']['file']) if t == 1 and p==t]\n",
    "for f in positive_hits_files:\n",
    "    shutil.copy(f, f\"found/{Path(f).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "positive_miss_files = [f for p, t, f in zip(predicted_labels, dataset['test']['label'], dataset['test']['file']) if t == 1 and p!=t]\n",
    "for f in positive_miss_files:\n",
    "    shutil.copy(f, f\"not found/{Path(f).name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
