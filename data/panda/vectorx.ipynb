{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Features, Value, Audio, ClassLabel\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "paths = pd.read_csv('dataset/slices_train.csv').path.values\n",
    "labels = [Path(p).name.split('_')[0] for p in paths]\n",
    "n = Counter(labels)\n",
    "labels = [l for l in labels if n[l] > 20]\n",
    "label2id = {l: ix  for ix, l in enumerate(set(labels))}\n",
    "label2id['unk'] = max(i for i in label2id.values()) + 1\n",
    "id2label = {ix: l for l, ix in label2id.items()}\n",
    "names = sorted([(n, i) for n, i in label2id.items()], key=lambda x: x[1])\n",
    "names = [n for n, i in names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c58ed15a5d5a3dac\n",
      "Reusing dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-c58ed15a5d5a3dac/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8453a56b69794d02bae6ce4ce0cf9c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c517883f7a50493b93c067ea7eb0dbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36993 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6221e339ea44e58907310e5d5f5f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4648 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562b30103e8b434d92a1300e20ff6cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4586 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "feats = Features({\"path\": Value(\"string\"),\n",
    "                  \"audio\": Audio(sampling_rate=16_000),\n",
    "                  \"label\": ClassLabel(names=names)}\n",
    "                  )\n",
    "\n",
    "def _generate_examples(example, label2id: dict = label2id):\n",
    "        label = Path(example['path']).name.split('_')[0]\n",
    "        example['label'] = label2id.get(label, label2id['unk'])\n",
    "        example['audio'] = example['path']\n",
    "        return example\n",
    "\n",
    "with open('tags_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_files = {'train': 'dataset/slices_train.csv', 'test': 'dataset/slices_test.csv', 'val': 'dataset/slices_val.csv'}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset = dataset.remove_columns(column_names=['Unnamed: 0', 'split'])\n",
    "\n",
    "\n",
    "dataset = dataset.map(_generate_examples, features=feats)\n",
    "dataset = dataset.rename_column('path', 'file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfd7894abfa4b55b9ba1c08cd1d4e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a026606f0054197b7eb144e119e9e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc23e21dc71b45b7b356301311b9c903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import Wav2Vec2ForXVector, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "model_checkpoint = \"facebook/wav2vec2-base\"\n",
    "batch_size = 32\n",
    "max_duration = 1\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=feature_extractor.sampling_rate, \n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration), \n",
    "        truncation=True, \n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForXVector: ['quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.weight', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForXVector from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForXVector from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForXVector were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['tdnn.0.kernel.weight', 'tdnn.3.kernel.bias', 'feature_extractor.weight', 'objective.weight', 'feature_extractor.bias', 'tdnn.0.kernel.bias', 'tdnn.4.kernel.weight', 'tdnn.3.kernel.weight', 'classifier.weight', 'tdnn.1.kernel.bias', 'tdnn.1.kernel.weight', 'tdnn.2.kernel.weight', 'tdnn.4.kernel.bias', 'projector.bias', 'projector.weight', 'classifier.bias', 'tdnn.2.kernel.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = Wav2Vec2ForXVector.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-xvector\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=35,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"f1\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n",
       "    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n",
       "    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n",
       "\n",
       "        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n",
       "        - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
       "        - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
       "        - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n",
       "        - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    f1 (`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple binary example\n",
       "        >>> f1_metric = datasets.load_metric(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'f1': 0.5}\n",
       "\n",
       "    Example 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\n",
       "        >>> f1_metric = datasets.load_metric(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.67\n",
       "\n",
       "    Example 3-The same simple binary example as in Example 1, but with `sample_weight` included.\n",
       "        >>> f1_metric = datasets.load_metric(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.35\n",
       "\n",
       "    Example 4-A multiclass example, with different values for the `average` input.\n",
       "        >>> predictions = [0, 2, 1, 0, 0, 1]\n",
       "        >>> references = [0, 1, 2, 0, 1, 2]\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.27\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.33\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.27\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\n",
       "        >>> print(results)\n",
       "        {'f1': array([0.8, 0. , 0. ])}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "metric = load_metric(\"f1\", cache_dir='/home/jovyan/.cache/huggingface/metrics')\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    logits = eval_pred.predictions[0]\n",
    "    proj = model.objective._parameters['weight'].cpu().detach().numpy()\n",
    "    predicted_labels = np.argmax(np.dot(logits, proj), axis=1)\n",
    "    res = metric.compute(predictions=predicted_labels, references=eval_pred.label_ids, average='weighted')\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset[\"val\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 36993\n",
      "  Num Epochs = 35\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 10115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10115' max='10115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10115/10115 6:58:41, Epoch 34/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.972000</td>\n",
       "      <td>9.282754</td>\n",
       "      <td>0.494746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.730100</td>\n",
       "      <td>6.484269</td>\n",
       "      <td>0.689371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.968000</td>\n",
       "      <td>4.822059</td>\n",
       "      <td>0.795504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.182500</td>\n",
       "      <td>3.685891</td>\n",
       "      <td>0.832956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.352900</td>\n",
       "      <td>3.038280</td>\n",
       "      <td>0.865561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.857300</td>\n",
       "      <td>2.449143</td>\n",
       "      <td>0.896701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.967500</td>\n",
       "      <td>2.071967</td>\n",
       "      <td>0.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.039100</td>\n",
       "      <td>1.890948</td>\n",
       "      <td>0.914620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.665400</td>\n",
       "      <td>1.711604</td>\n",
       "      <td>0.927954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.360600</td>\n",
       "      <td>1.603896</td>\n",
       "      <td>0.932025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.303300</td>\n",
       "      <td>1.562655</td>\n",
       "      <td>0.936823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.261800</td>\n",
       "      <td>1.522344</td>\n",
       "      <td>0.936481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.766600</td>\n",
       "      <td>1.525429</td>\n",
       "      <td>0.938809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.663100</td>\n",
       "      <td>1.264167</td>\n",
       "      <td>0.948110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.854700</td>\n",
       "      <td>1.188033</td>\n",
       "      <td>0.953547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.729100</td>\n",
       "      <td>1.124112</td>\n",
       "      <td>0.956645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.399600</td>\n",
       "      <td>1.002161</td>\n",
       "      <td>0.960203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.354000</td>\n",
       "      <td>0.957133</td>\n",
       "      <td>0.960323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.482800</td>\n",
       "      <td>0.940949</td>\n",
       "      <td>0.964454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.262700</td>\n",
       "      <td>0.937568</td>\n",
       "      <td>0.964595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.270600</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.964520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.222900</td>\n",
       "      <td>0.820089</td>\n",
       "      <td>0.969124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.948800</td>\n",
       "      <td>0.863048</td>\n",
       "      <td>0.966118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.966520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.781341</td>\n",
       "      <td>0.971060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.927600</td>\n",
       "      <td>0.751091</td>\n",
       "      <td>0.971370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.967400</td>\n",
       "      <td>0.775151</td>\n",
       "      <td>0.970824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.725526</td>\n",
       "      <td>0.974405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.725868</td>\n",
       "      <td>0.972901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.701442</td>\n",
       "      <td>0.972335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>0.698346</td>\n",
       "      <td>0.973509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.663300</td>\n",
       "      <td>0.702570</td>\n",
       "      <td>0.974114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>0.699597</td>\n",
       "      <td>0.973188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.736900</td>\n",
       "      <td>0.693265</td>\n",
       "      <td>0.973221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.686031</td>\n",
       "      <td>0.972811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.49474644990778804}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-289\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-289/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-289/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-289/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.6893705687016398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-578\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-578/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-578/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-578/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.7955035044782507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-867\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-867/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-867/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-867/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.8329559775436511}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1156\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1156/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.865561052739612}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1445\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1445/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.8967009390545976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-1734\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-1734/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9097000463410372}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2023\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2023/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9146199620187198}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2312\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2312/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9279538241747127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2601\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2601/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9320251208426955}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-2890\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-2890/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9368229765369066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3179\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3179/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9364810793693422}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3468\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3468/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9388092404231434}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-3757\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-3757/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9481100323068246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4046\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4046/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9535473362353735}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4335\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4335/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9566449639826673}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4624\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4624/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9602031926260322}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-4913\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-4913/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9603232290773355}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5202\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5202/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9644543911689891}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5491\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5491/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9645954077460133}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-5780\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-5780/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9645197916695856}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6069\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6069/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9691239255185394}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6358\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6358/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9661176529231834}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6647\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6647/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9665197187638916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-6936\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-6936/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9710602936387585}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7225\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7225/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9713701299682312}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7514\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7514/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9708238623533595}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-7803\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-7803/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9744048074888573}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8092\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8092/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.972901115518863}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8381\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8381/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9723351189232675}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8670\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8670/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9735086360418925}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-8959\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-8959/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9741139097461785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9248\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9248/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9731876791817936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9537\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9537/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9732213803231736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-9826\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-9826/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4586\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9728109377883826}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/checkpoint-10115\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/checkpoint-10115/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from wav2vec2-base-finetuned-xvector/checkpoint-8092 (score: 0.9744048074888573).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10115, training_loss=2.4172881723452013, metrics={'train_runtime': 25124.955, 'train_samples_per_second': 51.533, 'train_steps_per_second': 0.403, 'total_flos': 1.2024582268257595e+19, 'train_loss': 2.4172881723452013, 'epoch': 35.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-xvector/best_checkpoint\n",
      "Configuration saved in wav2vec2-base-finetuned-xvector/best_checkpoint/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-xvector/best_checkpoint/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-xvector/best_checkpoint/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.save_model(f\"{model_name}-finetuned-xvector/best_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from wav2vec2-base-finetuned-xvector/best_checkpoint.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer._load_from_checkpoint(f\"{model_name}-finetuned-xvector/best_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4648\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9685338664124005}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8347758054733276,\n",
       " 'test_f1': 0.9685338664124005,\n",
       " 'test_runtime': 37.9534,\n",
       " 'test_samples_per_second': 122.466,\n",
       " 'test_steps_per_second': 3.847}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "inputs = encoded_dataset['test']\n",
    "\n",
    "with torch.no_grad():\n",
    "        result = trainer.predict(test_dataset = inputs)\n",
    "result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chicken', 1.0, 1811),\n",
       " ('orange', 1.0, 346),\n",
       " ('rice', 0.9970149253731343, 335),\n",
       " ('entrees', 1.0, 204),\n",
       " ('entree', 1.0, 199),\n",
       " ('can', 0.9259259259259259, 162),\n",
       " ('honey', 1.0, 102),\n",
       " ('shrimp', 1.0, 98),\n",
       " ('steak', 1.0, 94),\n",
       " ('drinks', 1.0, 82),\n",
       " ('steamed', 1.0, 70),\n",
       " ('unk', 0.7246376811594203, 69),\n",
       " ('chow', 0.8382352941176471, 68),\n",
       " ('tea', 1.0, 67),\n",
       " ('drink', 1.0, 66),\n",
       " ('plate', 0.8032786885245902, 61),\n",
       " ('side', 1.0, 58),\n",
       " ('mein', 0.9814814814814815, 54),\n",
       " ('beef', 0.8958333333333334, 48),\n",
       " ('lemonade', 1.0, 47),\n",
       " ('bowl', 0.9333333333333333, 45),\n",
       " ('large', 1.0, 41),\n",
       " ('small', 0.9411764705882353, 34),\n",
       " ('one', 0.6451612903225806, 31),\n",
       " ('greens', 1.0, 28),\n",
       " ('two', 0.6538461538461539, 26),\n",
       " ('fried', 0.76, 25),\n",
       " ('medium', 1.0, 24),\n",
       " ('walnut', 0.9090909090909091, 22),\n",
       " ('mushroom', 0.9473684210526315, 19),\n",
       " ('teriyaki', 0.9444444444444444, 18),\n",
       " ('kung', 0.5, 16),\n",
       " ('broccoli', 1.0, 16),\n",
       " ('pao', 0.8571428571428571, 14),\n",
       " ('meal', 0.9285714285714286, 14),\n",
       " ('instead', 1.0, 14),\n",
       " ('pepper', 0.7142857142857143, 14),\n",
       " ('coke', 1.0, 13),\n",
       " ('strawberry', 1.0, 13),\n",
       " ('black', 0.9166666666666666, 12),\n",
       " ('chickens', 1.0, 11),\n",
       " ('bigger', 0.9, 10),\n",
       " ('bowls', 0.8, 10),\n",
       " ('green', 0.8888888888888888, 9),\n",
       " ('veggie', 1.0, 8),\n",
       " ('raspberry', 1.0, 8),\n",
       " ('beijing', 0.875, 8),\n",
       " ('breast', 0.625, 8),\n",
       " ('regular', 0.75, 8),\n",
       " ('milk', 0.625, 8),\n",
       " ('cheese', 0.5, 8),\n",
       " ('egg', 0.875, 8),\n",
       " ('grilled', 0.7142857142857143, 7),\n",
       " ('three', 0.5714285714285714, 7),\n",
       " ('plates', 0.8333333333333334, 6),\n",
       " ('super', 1.0, 6),\n",
       " ('angus', 1.0, 6),\n",
       " ('sides', 1.0, 6),\n",
       " ('white', 0.3333333333333333, 6),\n",
       " ('water', 1.0, 5),\n",
       " ('veggies', 1.0, 5),\n",
       " ('diet', 1.0, 3),\n",
       " ('rangoons', 0.6666666666666666, 3),\n",
       " ('vegetables', 1.0, 2),\n",
       " ('bottle', 1.0, 1)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from collections import Counter\n",
    "def get_predicted_labels(logits):\n",
    "    proj = model.objective._parameters['weight'].cpu().detach().numpy()\n",
    "    return np.argmax(np.dot(logits, proj), axis=1)\n",
    "    \n",
    "predicted_labels = get_predicted_labels(result.predictions[0])\n",
    "hits = [(t, p==t) for p, t in zip(predicted_labels, result.label_ids)]\n",
    "\n",
    "per_label_acc = {}\n",
    "for l, h in hits:\n",
    "    if h:\n",
    "        per_label_acc[l] = per_label_acc.get(l, 0) + 1\n",
    "accs = []\n",
    "for k, v in per_label_acc.items():\n",
    "    n = Counter(result.label_ids)[k]\n",
    "    accs.append((id2label[k], round(v/n, 2), n))\n",
    "sorted(accs, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4648/4648 [14:14<00:00,  5.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('water', 'water'),\n",
       " ('water', 'water'),\n",
       " ('water', 'water'),\n",
       " ('small', 'small'),\n",
       " ('small', 'small')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarities = []\n",
    "for ix, emb in tqdm(enumerate(result.predictions[1]), total=len(result.predictions[1])):\n",
    "    max_sim = 0\n",
    "    max_label = None\n",
    "    for jx, emb_2 in enumerate(result.predictions[1]):\n",
    "        if ix == jx:\n",
    "            continue\n",
    "        # sim = dot(emb, emb_2)/(norm(emb)*norm(emb_2))\n",
    "        sim = 1 - spatial.distance.cosine(emb, emb_2)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            max_label = id2label[result.label_ids[jx]]\n",
    "    l = id2label[result.label_ids[ix]]\n",
    "    similarities.append((l, max_label))\n",
    "similarities[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9677280550774526"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "sum([t==p for t, p in similarities])/len(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chicken', 1.0, 1811),\n",
       " ('orange', 1.0, 346),\n",
       " ('rice', 0.99, 335),\n",
       " ('entrees', 1.0, 204),\n",
       " ('entree', 1.0, 199),\n",
       " ('can', 0.9, 162),\n",
       " ('honey', 1.0, 102),\n",
       " ('shrimp', 1.0, 98),\n",
       " ('steak', 1.0, 94),\n",
       " ('drinks', 1.0, 82),\n",
       " ('steamed', 1.0, 70),\n",
       " ('unk', 0.64, 69),\n",
       " ('chow', 0.9, 68),\n",
       " ('tea', 1.0, 67),\n",
       " ('drink', 1.0, 66),\n",
       " ('plate', 0.84, 61),\n",
       " ('side', 1.0, 58),\n",
       " ('mein', 0.94, 54),\n",
       " ('beef', 0.88, 48),\n",
       " ('lemonade', 1.0, 47),\n",
       " ('bowl', 0.93, 45),\n",
       " ('large', 1.0, 41),\n",
       " ('small', 0.94, 34),\n",
       " ('one', 0.61, 31),\n",
       " ('greens', 0.96, 28),\n",
       " ('two', 0.73, 26),\n",
       " ('fried', 0.8, 25),\n",
       " ('medium', 0.96, 24),\n",
       " ('walnut', 0.91, 22),\n",
       " ('mushroom', 0.89, 19),\n",
       " ('teriyaki', 0.94, 18),\n",
       " ('kung', 0.56, 16),\n",
       " ('broccoli', 1.0, 16),\n",
       " ('pao', 0.86, 14),\n",
       " ('meal', 0.93, 14),\n",
       " ('instead', 1.0, 14),\n",
       " ('pepper', 0.71, 14),\n",
       " ('coke', 1.0, 13),\n",
       " ('strawberry', 1.0, 13),\n",
       " ('black', 0.92, 12),\n",
       " ('chickens', 1.0, 11),\n",
       " ('bigger', 0.9, 10),\n",
       " ('bowls', 0.8, 10),\n",
       " ('green', 0.78, 9),\n",
       " ('veggie', 1.0, 8),\n",
       " ('raspberry', 1.0, 8),\n",
       " ('beijing', 0.88, 8),\n",
       " ('breast', 0.88, 8),\n",
       " ('regular', 0.88, 8),\n",
       " ('milk', 1.0, 8),\n",
       " ('cheese', 0.62, 8),\n",
       " ('egg', 0.88, 8),\n",
       " ('grilled', 0.71, 7),\n",
       " ('three', 0.57, 7),\n",
       " ('plates', 1.0, 6),\n",
       " ('super', 1.0, 6),\n",
       " ('angus', 1.0, 6),\n",
       " ('sides', 0.83, 6),\n",
       " ('white', 0.17, 6),\n",
       " ('water', 1.0, 5),\n",
       " ('veggies', 0.8, 5),\n",
       " ('diet', 1.0, 3),\n",
       " ('rangoons', 0.33, 3),\n",
       " ('vegetables', 1.0, 2)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "per_label_acc = {}\n",
    "for t, p in similarities:\n",
    "    if t==p:\n",
    "        per_label_acc[t] = per_label_acc.get(t, 0) + 1\n",
    "accs = []\n",
    "for k, v in per_label_acc.items():\n",
    "    n = Counter(result.label_ids)[label2id[k]]\n",
    "    accs.append((k, round(v/n, 2), n))\n",
    "sorted(accs, key=lambda x: x[2], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
