{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: training\n",
    "output-file: training.html\n",
    "description: Train wav2vec model\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.22.0.dev0\n",
      "Uninstalling transformers-4.22.0.dev0:\n",
      "  Successfully uninstalled transformers-4.22.0.dev0\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# %%capture\n",
    "\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip uninstall transformers -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "from typing import List, Callable\n",
    "from nbdev.showdoc import *\n",
    "from IPython.display import display,SVG\n",
    "import numpy as np\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from wav2keyword.datasets_tools import dataloader_pipeline\n",
    "from wav2keyword.preprocesses import Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "        'evaluation_strategy': \"epoch\",\n",
    "        'save_strategy': \"epoch\",\n",
    "        'learning_rate': 3e-5,\n",
    "        'per_device_train_batch_size': 32,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'per_device_eval_batch_size': 32,\n",
    "        'num_train_epochs': 5,\n",
    "        'warmup_ratio': 0.1,\n",
    "        'logging_steps': 10,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': \"accuracy\",\n",
    "        'push_to_hub': False}\n",
    "\n",
    "class W2KTrainer(object):\n",
    "\n",
    "    def __init__(self, model_checkpoint: str = \"facebook/wav2vec2-base\", metric: str = 'accuracy'):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.model_name = model_checkpoint.split(\"/\")[-1]\n",
    "        self.training_args = TRAINING_ARGS\n",
    "        self.training_args['output_dir'] = f\"{self.model_name}-finetuned-ks\"\n",
    "        print(\"loading metric\")\n",
    "        self.metric = load_metric(metric)\n",
    "        self.preprocessor = Preprocessor(self.model_checkpoint)\n",
    "\n",
    "    def _get_model(self, id2label: dict, label2id: dict):\n",
    "        num_labels = len(id2label)\n",
    "        model = AutoModelForAudioClassification.from_pretrained(\n",
    "            self.model_checkpoint, \n",
    "            num_labels=num_labels,\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_training_args(self, training_kwargs = None):\n",
    "        training_args = self.training_args.copy()\n",
    "        if training_kwargs:\n",
    "            for k, v in training_kwargs.items():\n",
    "                training_args[k] = v\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            **training_args\n",
    "        )\n",
    "        return args\n",
    "\n",
    "    def _compute_metrics(self, eval_pred):\n",
    "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return self.metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "    def build_trainer(self, dataset, id2label, label2id, args = None, preprocess_kwargs: dict = {'max_duration': 1.0}):\n",
    "        encoded_dataset = self.preprocessor.preprocess(dataset, fn_kwargs = preprocess_kwargs)\n",
    "        trainer = Trainer(\n",
    "            self._get_model(id2label, label2id),\n",
    "            self.get_training_args(args),\n",
    "            train_dataset=encoded_dataset[\"train\"],\n",
    "            eval_dataset=encoded_dataset[\"validation\"],\n",
    "            tokenizer=self.preprocessor.FEATURE_EXTRACTOR,\n",
    "            compute_metrics=self._compute_metrics\n",
    "        )\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### W2KTrainer.get_training_args\n",
       "\n",
       ">      W2KTrainer.get_training_args (training_kwargs=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### W2KTrainer.get_training_args\n",
       "\n",
       ">      W2KTrainer.get_training_args (training_kwargs=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(W2KTrainer.get_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### W2KTrainer.build_trainer\n",
       "\n",
       ">      W2KTrainer.build_trainer (dataset, id2label, label2id, args=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### W2KTrainer.build_trainer\n",
       "\n",
       ">      W2KTrainer.build_trainer (dataset, id2label, label2id, args=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(W2KTrainer.build_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset superb (/home/jovyan/.cache/huggingface/datasets/superb/ks/1.9.0/ce836692657f82230c16b3bbcb93eaacdbfd7de4def3be90016f112d68683481)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e771eeb614409ea65880b6b368ad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|filter_stream Reusing\n",
    "#|filter_stream UserWarning\n",
    "#| eval: false\n",
    "\n",
    "data = dataloader_pipeline({'path': \"superb\", 'name': \"ks\"})\n",
    "dataset = data['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the pretrained model and fine-tune it. `W2KTrainer` will use the `AutoModelForAudioClassification` class. Like with the feature extractor, the `from_pretrained` method in `W2KTrainer` will download and cache the model for us. As the label ids and the number of labels are dataset dependent, we pass `num_labels`, `label2id`, and `id2label` alongside the dataset here.\n",
    "\n",
    "To instantiate a `Trainer`, we will need to define the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. `W2KTrainer` has a default `TrainingArguments` setup, but you can override any number of those parameters by passing a dictionary with them over the `args` argument.  \n",
    "Since we are using the default `TrainingArguments`, we are not passing any custom args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:362: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function Preprocessor._preprocess_function> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db341e7023b43e5be04491e3b9e0142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a096bb7d2c4340b88f7563428aa882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8ed6a843854cd88636c35ec0c3a660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:362: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.weight', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#|filter_stream UserWarning|_preprocess_function|VisibleDeprecationWarning\n",
    "#| eval: false\n",
    "\n",
    "w2ktrainer = W2KTrainer()\n",
    "trainer = w2ktrainer.build_trainer(dataset, data['id2label'], data['label2id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the `quantizer` and `project_q` layers) and randomly initializing some other (the `projector` and `classifier` layers). This is expected in this case, because we are removing the head used to pretrain the model on an unsupervised Vector Quantization objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will review the default `TrainingArguments` before continuing. We set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` of 32 and set the number of epochs for training in 5, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved (according to `metric_name`) at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_strategy': 'epoch',\n",
       " 'save_strategy': 'epoch',\n",
       " 'learning_rate': 3e-05,\n",
       " 'per_device_train_batch_size': 32,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'per_device_eval_batch_size': 32,\n",
       " 'num_train_epochs': 5,\n",
       " 'warmup_ratio': 0.1,\n",
       " 'logging_steps': 10,\n",
       " 'load_best_model_at_end': True,\n",
       " 'metric_for_best_model': 'accuracy',\n",
       " 'push_to_hub': False,\n",
       " 'output_dir': 'wav2vec2-base-finetuned-ks'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "w2ktrainer.training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W2KTrainer` defines an internal method `_compute_metrics` for how to compute the metrics from the predictions, which will just use the metric passed during instantiation which defaults to `accuracy`.  \n",
    "The only preprocessing it has to do is to take the argmax of our predicted logits. This is all done internally to instantiate the `Trainer`, but we show it here for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"accuracy\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions: Predicted labels, as returned by a model.\n",
       "    references: Ground truth labels.\n",
       "    normalize: If False, return the number of correctly classified samples.\n",
       "        Otherwise, return the fraction of correctly classified samples.\n",
       "    sample_weight: Sample weights.\n",
       "Returns:\n",
       "    accuracy: Accuracy score.\n",
       "Examples:\n",
       "\n",
       "    >>> accuracy_metric = datasets.load_metric(\"accuracy\")\n",
       "    >>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "w2ktrainer.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### W2KTrainer._compute_metrics\n",
       "\n",
       ">      W2KTrainer._compute_metrics (eval_pred)\n",
       "\n",
       "Computes accuracy on a batch of predictions"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### W2KTrainer._compute_metrics\n",
       "\n",
       ">      W2KTrainer._compute_metrics (eval_pred)\n",
       "\n",
       "Computes accuracy on a batch of predictions"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "show_doc(W2KTrainer._compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finetune our model by calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 51094\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1995\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1995' max='1995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1995/1995 1:20:59, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.567452</td>\n",
       "      <td>0.953074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.175072</td>\n",
       "      <td>0.976317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.116128</td>\n",
       "      <td>0.980141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.094171</td>\n",
       "      <td>0.979847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.090120</td>\n",
       "      <td>0.981906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-399\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-399/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-399/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-399/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-798\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-798/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-798/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-798/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1197\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1197/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1197/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1197/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1596\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1596/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1596/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1596/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-1995\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/checkpoint-1995/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/checkpoint-1995/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-1995/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from wav2vec2-base-finetuned-ks/checkpoint-1995 (score: 0.9819064430714917).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1995, training_loss=0.4566893815097952, metrics={'train_runtime': 4864.0932, 'train_samples_per_second': 52.522, 'train_steps_per_second': 0.41, 'total_flos': 2.31918157475328e+18, 'train_loss': 0.4566893815097952, 'epoch': 5.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6798\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09011975675821304,\n",
       " 'eval_accuracy': 0.9819064430714917,\n",
       " 'eval_runtime': 62.8135,\n",
       " 'eval_samples_per_second': 108.225,\n",
       " 'eval_steps_per_second': 3.391,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we export the best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-base-finetuned-ks/best_checkpoint\n",
      "Configuration saved in wav2vec2-base-finetuned-ks/best_checkpoint/config.json\n",
      "Model weights saved in wav2vec2-base-finetuned-ks/best_checkpoint/pytorch_model.bin\n",
      "Feature extractor saved in wav2vec2-base-finetuned-ks/best_checkpoint/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "trainer.save_model(f\"{w2ktrainer.training_args['output_dir']}/best_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
