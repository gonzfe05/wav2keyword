{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def chunk_audio(audio: AudioSegment, increment_mills: int = 300, windows: int = 1000) -> List[AudioSegment]:\n",
    "    sample_rate = audio.frame_count(ms=1000)\n",
    "    seconds = audio.frame_count()/sample_rate\n",
    "    def get_segment(e, window):\n",
    "        return audio[e-windows:e]\n",
    "    audio_segments = Parallel(n_jobs=4)(delayed(get_segment)(e, windows) for e in np.arange(windows, seconds*1000, increment_mills))\n",
    "    return audio_segments\n",
    "\n",
    "def export_audio(segment: AudioSegment, path: str):\n",
    "    return segment.export(f'{path}.wav', format=\"wav\")\n",
    "\n",
    "def chunk_selected_audios(audios: List[str], output_path: str) -> None:\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    for file in audios:\n",
    "        audio = AudioSegment.from_wav(file)\n",
    "        for ix, segment in enumerate(chunk_audio(audio)):\n",
    "            path = os.path.join(output_path, f\"{Path(file).stem}_{ix}.wav\")\n",
    "            export_audio(segment, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14346.99s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "14347.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "14347.01s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "14347.02s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('data/panda/dataset/test.csv', index_col=0)\n",
    "\n",
    "for group_name, df_group in test.groupby('label'):\n",
    "    output_path = os.path.join('chunks', group_name)\n",
    "    audios = df_group.path.values\n",
    "    chunk_selected_audios(audios, output_path)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3449389385.py, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [30]\u001b[0;36m\u001b[0m\n\u001b[0;31m    audio_bytes = bytearray*\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import Wav2Vec2ForXVector, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from pydantic import BaseModel\n",
    "from io import BytesIO\n",
    "\n",
    "class AudioArray(BaseModel):\n",
    "    array: List[float]\n",
    "\n",
    "class XvectorInput(BaseModel):\n",
    "    model_input: List[List[float]]\n",
    "\n",
    "class XvectorModel(object):\n",
    "\n",
    "    def __init__(self, model_checkpoint: str) -> None:\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.model = Wav2Vec2ForXVector.from_pretrained(self.model_checkpoint)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(self.model_checkpoint)\n",
    "    \n",
    "    def preprocess_function(self, audio_arrays: List[AudioArray], max_duration: float = 1.0)\\\n",
    "         -> List[List[float]]:\n",
    "        arrays = [a.array for a in audio_arrays]\n",
    "        inputs = self.feature_extractor(\n",
    "            arrays,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate, \n",
    "            max_length=int(self.feature_extractor.sampling_rate * max_duration),\n",
    "            truncation=True\n",
    "            )\n",
    "        return inputs\n",
    "    \n",
    "    def parse_audio(self, raw_data: bytes, sample_width: int, channels: int, frame_rate: int) -> List[AudioArray]:\n",
    "        audio = AudioSegment.from_raw(\n",
    "            BytesIO(raw_data), \n",
    "            sample_width=sample_width,\n",
    "            channels=channels,\n",
    "            frame_rate=frame_rate\n",
    "        )\n",
    "        result = []\n",
    "        for ix, segment in enumerate(chunk_audio(audio)):\n",
    "            result.append(segment)\n",
    "        return result\n",
    "\n",
    "    def encode_data(self, audio_arrays: List[AudioArray]) -> XvectorInput:\n",
    "        result = {'input_values': self.preprocess_function(audio_arrays)}\n",
    "        return XvectorInput.parse_obj(result)\n",
    "    \n",
    "    def get_logits(self, inputs: XvectorInput):\n",
    "        with torch.no_grad():\n",
    "            result = self.model(**inputs.dict())\n",
    "        return result\n",
    "    \n",
    "    def get_predicted_labels(self, logits):\n",
    "        proj = self.model.objective._parameters['weight'].cpu().detach().numpy()\n",
    "        return np.argmax(np.dot(logits, proj), axis=1)\n",
    "\n",
    "model_checkpoint = 'data/panda/wav2vec2-base-finetuned-xvector/best_checkpoint/'\n",
    "# xvector_model = XvectorModel(model_checkpoint)\n",
    "\n",
    "file = '/home/jovyan/.cache/panda/audios/1655690608-SIP-A90CCE12F2CF-000041c0-chunk4.wav_3.wav'\n",
    "with open(file, \"rb\") as f:\n",
    "    audio_bytes = bytearray()\n",
    "    while (byte := f.read(1)):\n",
    "        audio_bytes.append(byte)\n",
    "    xvector_model.parse_audio(audio_bytes, 2, 1, 16000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Union[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadadsa/sdad/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "audios_path = f'{Path.home()}/.cache/panda/audios'\n",
    "audios = glob(f'{audios_path}/*.wav')\n",
    "chunk_selected_audios(audios[:5], 'test/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
